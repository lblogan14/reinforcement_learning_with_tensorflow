{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch1_deep_learning_review.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/reinforcement_learning_with_tensorflow/blob/master/ch1_deep_learning_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ii4afl976nnH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Activation functions for deep learning\n",
        "decide whether a particular neuron is activated or not, and whether the information received by the nueron is relevant or not.\n",
        "\n",
        "##Sigmoid function\n",
        "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch1/sigmoid.PNG?raw=true)\n",
        "\n",
        "Observing the curve of the function, we see that the gradient is very highwhen $x$ values between -3 and 3, but becomes flat beyond that. Thus, we can say that smallchanges in $x$ near these points will bring large changes in the value of the sigmoid function.\n",
        "\n",
        "Therefore, the function goals in pushing the values of the sigmoid function towards the extremes. This is why the sigmoid function is being used in classification problems.\n",
        "\n",
        "The gradient of the sigmoid functions is shown below and it is a smooth curve dependent on $x$, so it is easy to backpropagate the error and update the parameters, $w$ and $b$.\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch1/sigmoid_grad.PNG?raw=true)\n",
        "\n",
        "Sigmoid disadvantage: whenever the function falls beyond +3 or below -3 region, the gradients tends to approach zero and the learning of neural network comes to a halt."
      ]
    },
    {
      "metadata": {
        "id": "uqk8Cg2Gl_CO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##tanh function"
      ]
    },
    {
      "metadata": {
        "id": "rFVIYYXnmEoi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unlike the sigmoid function, the tanh function is a continuous function symmetric around the origin, ranging from -1 to 1.\n",
        "$$\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch1/tanh.PNG?raw=true)\n",
        "\n",
        "It is continuous and also non-linear plus differentiable at all points. Although symmetrical, it becomes flat beyond -2 and 2.\n",
        "\n",
        "The gradient of the tanh function is shown below and it is steeper than the sigmoid. Also, the tanh function also has the vanishing gradient problem.\n",
        "\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch1/tanh_grad.PNG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "s3mN8VamnWNU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Softmax function\n",
        "mainly used to handle classification problems and preferably used in the output layer, outputting the probabilities of the output classes.\n",
        "$$\\mathrm{softmax}(x_i)=\\frac{e^{x_i}}{\\sum\\limits_j^ce^{x_j}}$$\n",
        "While the sigmoid function was able to handle only two classes, the softmax function can handle multi-class to generate values for all the classes and follow the rules of probability.\n",
        "\n",
        "For example, $x\\in[1,2,3,4]$, where $x$ refers to four classes, then the softmax function will provide:\n",
        "$$\\mathrm{softmax}(x_1)=\\frac{e^{x_1}}{\\sum\\limits_j^4e^{x_j}}=\\frac{e^1}{e^1+e^2+e^3+e^4}=0.032$$\n",
        "$$\\mathrm{softmax}(x_2)=\\frac{e^{x_2}}{\\sum\\limits_j^4e^{x_j}}=\\frac{e^2}{e^1+e^2+e^3+e^4}=0.088$$\n",
        "$$\\mathrm{softmax}(x_3)=\\frac{e^{x_3}}{\\sum\\limits_j^4e^{x_j}}=\\frac{e^3}{e^1+e^2+e^3+e^4}=0.240$$\n",
        "$$\\mathrm{softmax}(x_4)=\\frac{e^{x_4}}{\\sum\\limits_j^4e^{x_j}}=\\frac{e^4}{e^1+e^2+e^3+e^4}=0.640$$\n",
        "Thus, these are the probablities of all the classes.\n",
        "\n",
        "The softmax function shrinks the outputs for each class between 0 and 1 and divides them by the sum of the outputs for all the classes. This is why the softmax function becomes the best candidate for the outer layer activation function of the classifier."
      ]
    },
    {
      "metadata": {
        "id": "7qtl6gl3rIsx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Rectified linear unit (ReLU) function\n",
        "$$\\mathrm{ReLU}(x)=\\max(0,x)$$\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch1/relu.PNG?raw=true)\n",
        "\n",
        "ReLU is non-linear and is easy to backpropagate. Therefore, it stack multiple hidden layers activated.\n",
        "\n",
        "Compared to other activation functions, ReLU does not activate all the neurons at the same time. If the input is negative it outputs zero and the neuron does not activate, which results in a sparse network, fast and easy computation.\n",
        "\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch1/relu_grad.PNG?raw=true)\n",
        "\n",
        "The negative side of the gradient of ReLU shows the constant zero. Therefore, activations falling in that region will have zero gradients and weights are not updated, so the nodes/neurons are inactive, as they will not learn. To overcome this problem, the Leaky ReLU is introduced:\n",
        "$$\\mathrm{LeakyReLU}(x)=\n",
        "    \\left\\{\n",
        "        \\begin{array}{ll}\n",
        "            ax & \\quad x \\leq 0 \\\\\n",
        "            x & \\quad x > 0\n",
        "        \\end{array}\n",
        "    \\right.$$\n",
        "This prevents the gradient from becoming zero in the negative side and the weight training continues, but slowly, owing to the low value of $a$."
      ]
    },
    {
      "metadata": {
        "id": "B7pmxhKXtpzM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Choose the right activation function\n",
        "* Sigmoid functions work very well in the case of shallow networks and binary\n",
        "classifiers. Deeper networks may lead to vanishing gradients.\n",
        "* The ReLU function is the most widely used, and try using Leaky ReLU to avoid\n",
        "the case of dead neurons. Thus, start with ReLU, then move to another activation\n",
        "function if ReLU doesn't provide good results.\n",
        "* Use softmax in the outer layer for the multi-class classification.\n",
        "* Avoid using ReLU in the outer layer."
      ]
    },
    {
      "metadata": {
        "id": "n6pDm3Y5u8vA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Limitations of deep learning\n",
        "\n",
        "##Vanishing gradient problem\n",
        "Neurons present in the early layers are not able to learn because the gradients that train the weights shrink down to zero. This happens due to the\n",
        "greater depth of neural network, along with activation functions with derivatives resulting\n",
        "in low value.\n",
        "\n",
        "## Exploding gradient problem\n",
        "The learning of the neurons present in the early layers diverge\n",
        "because the gradients become too large to cause severe changes in weights avoiding\n",
        "convergence. This generally happens if weights are not assigned properly.\n",
        "\n",
        "##Overcoming the limitations of deep learning\n",
        "* Minimizing the use of the sigmoid and tanh activation functions\n",
        "* Using a momentum-based stochastic gradient descent\n",
        "* Proper initialization of weights and biases, such as xavier initialization\n",
        "* Regularization (add regularization loss along with data loss and minimize that)"
      ]
    },
    {
      "metadata": {
        "id": "cq4TR0-Rv_d0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Reinforcement learning\n",
        "\n",
        "##Terminologies and conventions\n",
        "* **Agent**: This we create by programming such that it is able to sense the\n",
        "environment, perform actions, receive feedback, and try to maximize rewards.\n",
        "* **Environment**: The world where the agent resides. It can be real or simulated.\n",
        "* **State**: The perception or configuration of the environment that the agent senses.\n",
        "State spaces can be finite or infinite.\n",
        "* **Rewards**: Feedback the agent receives after any action it has taken. The goal of\n",
        "the agent is to maximize the overall reward, that is, the immediate and the future\n",
        "reward. Rewards are defined in advance. Therefore, they must be created\n",
        "properly to achieve the goal efficiently.\n",
        "* **Actions**: Anything that the agent is capable of doing in the given environment.\n",
        "Action space can be finite or infinite.\n",
        "* **SAR triple**: (state, action, reward) is referred as the SAR triple, represented as (s,\n",
        "a, r).\n",
        "* **Episode**: Represents one complete run of the whole task.\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch1/rl.PNG?raw=true)\n",
        "\n",
        "Every task is a sequence of SAR triples. Start from state $S(t)$, perform action $A(t)$ and thereby, receive a reward $R(t+1)$, and land on a new state $S(t+1)$. The current state and\n",
        "action pair gives rewards for the next step. Since, $S(t)$ and $A(t)$ results in $S(t+1)$, we have a new triple of (current state, action, new state), that is, $[S(t),A(t),S(t+1)]$ or $(s,a,s')$.\n",
        "\n",
        "##Optimality criteria\n",
        "A measure of goodness of fit of the model created over the data.For example, in supervised classification learning algorithms, we have maximum likelihood\n",
        "as the optimality criteria. Thus, on the basis of the problem statement and objective\n",
        "optimality criteria differs.\n",
        "\n",
        "THe major goal in reinforcement learning is to maximize the future rewards. There are two different optimality criteria:\n",
        "* **Value function**: To quantify a state on the basis of future probable rewards\n",
        "* **Policy**: To guide an agent on what action to take in a given state\n",
        "\n",
        "###Value function for optimality\n",
        "The immediate and future rewards should be considered. Therefore, a value is assigned to each encountered state that reflectes this future information. This is called **value function**. The **delayed reward** is introduced here to explain what actions taken now will lead to potential rewards in future.\n",
        "\n",
        "$V(s)$, the value of the state, is defined as the expected value of rewards to be received in\n",
        "future for all the actions taken from this state to subsequent states until the agent reaches\n",
        "the goal state. Thus, the value functions tell how good it is to be in this state. The higher the value, the better the state.\n",
        "\n",
        "Rewards assigned to each $(s, a, s')$ triple is fixed, which is not the case with the value of the state; it is subjected to change with every action in the episode and with different episodes too.\n",
        "\n",
        "To save time comsuption and computation expense, the knowledge of the current state is stored in $V(s)$:\n",
        "$$V(s)=\\mathrm{E}[all\\,future\\,rewards\\,discounted | S(t)=s]$$"
      ]
    },
    {
      "metadata": {
        "id": "_YOX3dBsz-Ru",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Policy model for optimality\n",
        "**Policy** is defined as the model that guides the agent with action selection in different states.\n",
        "Policy is denoted as $\\pi$. $\\pi$ is basically the probability of a certain action given a particular state:\n",
        "$$\\pi(a,s)=p(A(t)=a|S(t)=s)$$\n",
        "\n",
        "The policy map provides the set of probabilities of different actions given a particular state. The policy along with the value function create a solution that helps in\n",
        "agent navigation as per the policy and the calculated value of the state."
      ]
    },
    {
      "metadata": {
        "id": "mTrzqD_90lic",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Q-learning approach to reinforcement learning\n",
        "Q-learning is an attempt to learn the value $Q(s,a)$ of a specific action given to the agent in a\n",
        "particular state.\n",
        "\n",
        "Consider a table where the number of rows represent the number of states,\n",
        "and the number of columns represent the number of actions. This is called a Q-table. Therefore, the agent needs to learn the value to find which action is the best in a given state.\n",
        "\n",
        "Steps for Q-learning:\n",
        "1. Initialize the table of $Q(s,a)$ with uniform values (say, all zeros).\n",
        "2. Observe the current state, $s$\n",
        "3. Choose an action, $a$, by epsilon greedy or any other action selection policies, and\n",
        "take the action\n",
        "4. As a result, $a$ reward, $r$, is received and a new state, $s'$, is perceived\n",
        "5. Update the $Q$ value of the $(s,a)$ pair in the table by using the following Bellman\n",
        "equation:\n",
        "$$Q(s,a)=r+\\gamma(\\max(Q(s',a')))$$\n",
        "  * $\\gamma$ is the discounting factor\n",
        "6. Then, set the value of current state as a new state and repeat the process to\n",
        "complete one episode, that is, reaches the terminal state\n",
        "7. Run multiple episodes to train the agent\n",
        "\n",
        "To simplify this concept:\n",
        "\n",
        "The Q-value for a given state, $s$, and action, $a$, is updated by the\n",
        "sum of current reward, $r$, and the discounted ($\\gamma$ ) maximum $Q$ value for the new state among all its actions. The discount factor delays the reward from the future compared to\n",
        "the present rewards. For example, a reward of 100 today will be worth more than 100 in the\n",
        "future. Similarly, a reward of 100 in the future must be worth less than 100 today. Thus, the future rewards are discounted. Repeating this update process continuously results in\n",
        "Q-table values converging to accurate measures of the expected future reward for a given\n",
        "action in a given state."
      ]
    },
    {
      "metadata": {
        "id": "3BCG3dwb2DfG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Asynchronous advantage actor-critic (A3C)\n",
        "The A3C algorithm was published in June 2016 by the combined team of Google DeepMind\n",
        "and MILA. It is simpler and has a lighter framework that used the asynchronous gradient\n",
        "descent to optimize the deep neural network. It was faster and was able to show good\n",
        "results on the multi-core CPU instead of GPU. One of A3C's big advantages is that it can\n",
        "work on continuous as well as discrete action spaces. As a result, it has opened the gateway\n",
        "for many new challenging problems that have complex state and action spaces.\n",
        "\n",
        "Basic overview of the algorithm:\n",
        "* **Asynchronous**: In Deep Q-Network, a neural network is used with the agent\n",
        "to predict actions. This means there is one agent and it's interacting with one\n",
        "environment. What A3C does is create multiple copies of the agent-environment\n",
        "to make the agent learn more efficiently. A3C has a global network, and multiple\n",
        "worker agents, where each agent has its own set of network parameters and each\n",
        "of them interact with their copy of the environment simultaneously without\n",
        "interacting with another agent's environment. The reason this works better than a\n",
        "single agent is that the experience of each agent is independent of the experience\n",
        "of the other agents. Thus, the overall experience from all the worker agents\n",
        "results in diverse training.\n",
        "* **Actor-critic**: Actor-critic combines the benefits of both value iteration and policy\n",
        "iteration. Thus, the network will estimate both a value function, $V(s)$, and a\n",
        "policy, $f(s)$, for a given state, $s$. There will be two separate fully-connected layers\n",
        "at the top of the function approximator neural network that will output the value\n",
        "and policy of the state, respectively. The agent uses the value, which acts as a\n",
        "critic to update the policy, that is, the intelligent actor.\n",
        "* **Advantage**: Policy gradients used discounted returns telling the agent whether\n",
        "the action was good or bad. Replacing that with Advantage not only quantifies\n",
        "the the good or bad status of the action but helps in encouraging and\n",
        "discouraging actions better."
      ]
    },
    {
      "metadata": {
        "id": "S9UdbjbZ3HCH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Basic computations in TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "wPaRLh7t3J18",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R3Y-7-S33L--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d574ac6-d8df-4863-da5d-9dda8e8d3143"
      },
      "cell_type": "code",
      "source": [
        "tf.zeros(3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'zeros:0' shape=(3,) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "_R3aJV7W3O3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "128020f4-8bfa-4670-da9f-a67239ea50d8"
      },
      "cell_type": "code",
      "source": [
        "tf.ones(3)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'ones:0' shape=(3,) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "3CmI3mGo3RbB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorFlow returns a reference to the tensor and not the value of the tensor.\n",
        "In order to get the value, we can use `eval()` or 'run()'\n",
        ", a function of tensor objects by\n",
        "running a session:"
      ]
    },
    {
      "metadata": {
        "id": "LvZD1dTa3cmo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.zeros(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n3Ms638G3eH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a55f6f6-e15b-4747-e4ba-2fe7179473ac"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(a)\n",
        "  print(a.eval())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dcG7OlzA3xq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tf.fill()` and `tf.constant()` methods create a tensor of a certain shape and value:"
      ]
    },
    {
      "metadata": {
        "id": "CKz4_3ZU3him",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.fill((2,2), value=4.)\n",
        "b = tf.constant(4., shape=(3,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrE-O9dm39fX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d35fa545-8761-4e06-9150-121de5a5c351"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(a)\n",
        "  sess.run(b)\n",
        "  print(a.eval())\n",
        "  print(b.eval())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4. 4.]\n",
            " [4. 4.]]\n",
            "[[4. 4. 4.]\n",
            " [4. 4. 4.]\n",
            " [4. 4. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "olr7XS0U4RF9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Random number initializers.\n",
        "* `tf.random_normal()`: Samples random values from the Normal distribution of\n",
        "specified mean and standard deviation\n",
        "* `tf.random_uniform()`: Samples random values from the Uniform distribution\n",
        "of a specified range"
      ]
    },
    {
      "metadata": {
        "id": "5NCNtAoJ4k5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.random_normal((4,4), mean=0, stddev=1)\n",
        "b = tf.random_uniform((2,2), minval=-3, maxval=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y8AHEk694t-L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "9b562f51-385f-41c2-bbb6-fac7701553d5"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(a)\n",
        "  sess.run(b)\n",
        "  print(a.eval())\n",
        "  print()\n",
        "  print(b.eval())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.3831449  -1.8989109  -0.60519767  1.0701469 ]\n",
            " [-1.3313991   2.2024977  -0.57883346  0.82913184]\n",
            " [ 0.08403209  0.08545087 -1.2015274   0.37987992]\n",
            " [ 1.303021    0.38369858 -0.2085839  -0.8417597 ]]\n",
            "\n",
            "[[-0.50290275  1.2167172 ]\n",
            " [ 0.61045575 -2.814738  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9NaXJQRC5G6W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tf.Variable()` defines the variables in TensorFlow which are holders for tensors"
      ]
    },
    {
      "metadata": {
        "id": "aA3DUE-d5OLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a0e44ff-0220-47d9-8a98-1f3d1268b55c"
      },
      "cell_type": "code",
      "source": [
        "a = tf.Variable(tf.ones(2,2))\n",
        "a"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(2,) dtype=float64_ref>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "mg304CMq5TuS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The evaluation fails in case of variables because they have to be explicitly initialized by\n",
        "using `tf.global_variables_initializer()` within a session:"
      ]
    },
    {
      "metadata": {
        "id": "xsAdMkRd5RM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0d62649-1ab5-4ef3-fd4f-7c5c82bf2dfe"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  print(a.eval())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1C-GAgfP5tSO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tf.eye()` defines the identity matrices"
      ]
    },
    {
      "metadata": {
        "id": "_aJOakJG5xyC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "cbafae92-aaf9-40e2-fc26-a1ebf584d14e"
      },
      "cell_type": "code",
      "source": [
        "id = tf.eye(4)\n",
        "with tf.Session() as sess:\n",
        "  sess.run(id)\n",
        "  print(id.eval())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ReBKIIF_6FAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tf.diag()` defines the diagonal matrices:"
      ]
    },
    {
      "metadata": {
        "id": "4PJrpR0s6Itt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.range(1,5,1)\n",
        "md = tf.diag(a)\n",
        "mdn = tf.diag([1,2,5,3,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1tH9bjRN6PKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "9c62a4fa-7995-48a6-a64e-9b200034df0a"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(md)\n",
        "  sess.run(mdn)\n",
        "  print(md.eval())\n",
        "  print()\n",
        "  print(mdn.eval())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0]\n",
            " [0 2 0 0]\n",
            " [0 0 3 0]\n",
            " [0 0 0 4]]\n",
            "\n",
            "[[1 0 0 0 0]\n",
            " [0 2 0 0 0]\n",
            " [0 0 5 0 0]\n",
            " [0 0 0 3 0]\n",
            " [0 0 0 0 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CVXSNv_Q6fnA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tf.transpose()` transposes the given matrix"
      ]
    },
    {
      "metadata": {
        "id": "giZm6GTN6jFx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.ones((2,3))\n",
        "b = tf.transpose(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbWjrsjD6o2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ffb412aa-bc0a-443d-dbc1-47024fee3b91"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(a)\n",
        "  sess.run(b)\n",
        "  print(a.eval())\n",
        "  print()\n",
        "  print(b.eval())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6pJwnXR56xZH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tf.matmul` defines the matrix multiplication"
      ]
    },
    {
      "metadata": {
        "id": "vv7bbKGA60bH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.ones((3,2))\n",
        "b = tf.ones((2,4))\n",
        "\n",
        "c = tf.matmul(a,b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AujeT1cS66-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "630b8310-5b41-4373-dc9b-14e6c807b40e"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(a)\n",
        "  sess.run(b)\n",
        "  sess.run(c)\n",
        "  \n",
        "  print('a:', a.eval())\n",
        "  print()\n",
        "  print('b:', b.eval())\n",
        "  print()\n",
        "  print('c:', c.eval())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: [[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]]\n",
            "\n",
            "b: [[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "\n",
            "c: [[2. 2. 2. 2.]\n",
            " [2. 2. 2. 2.]\n",
            " [2. 2. 2. 2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zn2Vndj37KXf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tf.reshape()` reshapes the tensors from one to another."
      ]
    },
    {
      "metadata": {
        "id": "TAktzQH77H85",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.ones((2,4))\n",
        "b = tf.reshape(a, (8,))\n",
        "# reshape it to a vector of size 8\n",
        "\n",
        "c = tf.reshape(a, (2,2,2))\n",
        "# reshape tensor a to shape (2,2,2)\n",
        "\n",
        "d = tf.reshape(b, (2,2,2))\n",
        "# reshape tensor b to shape (2,2,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "__WY1OEx7sXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "8b70d956-b64f-46a2-cbb4-533836bc22b6"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(a)\n",
        "  sess.run(b)\n",
        "  sess.run(c)\n",
        "  sess.run(d)\n",
        "  \n",
        "  print('a:', a.eval())\n",
        "  print('b:', b.eval())\n",
        "  print('c:', c.eval())\n",
        "  print('d:', d.eval())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: [[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "b: [1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "c: [[[1. 1.]\n",
            "  [1. 1.]]\n",
            "\n",
            " [[1. 1.]\n",
            "  [1. 1.]]]\n",
            "d: [[[1. 1.]\n",
            "  [1. 1.]]\n",
            "\n",
            " [[1. 1.]\n",
            "  [1. 1.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vEN9DUgg8JVo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The flow of computation in TensorFlow is represented as a computational graph, which is\n",
        "as instance of `tf.Graph()`. The graph contains tensors and operation objects, and keeps track\n",
        "of a series of operations and tensors involved.\n",
        "\n",
        "The default instance of the graph can be fetched by `tf.get_default_graph()`:"
      ]
    },
    {
      "metadata": {
        "id": "CRyWy1U68Xf7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "181c5a89-3c7d-45c3-dda6-4526a8eb16c9"
      },
      "cell_type": "code",
      "source": [
        "tf.get_default_graph()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.framework.ops.Graph at 0x7f7781422630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}