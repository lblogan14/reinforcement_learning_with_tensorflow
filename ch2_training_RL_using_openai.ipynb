{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch2_training_RL_using_openai.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/reinforcement_learning_with_tensorflow/blob/master/ch2_training_RL_using_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rR_dNS7T9yPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "450a702f-44fa-43bd-bffb-9f6d6772c779"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fJfV83Nx90mn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "32ebf9b2-7b2d-4b4f-ece0-fe7b953c388c"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My' 'Drive/Colab' 'Notebooks/Reinforcement_Learning_with_TensorFlow/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Reinforcement_Learning_with_TensorFlow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b1aUn0UnAA3q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Understand an OpenAI Gym Environment\n",
        "Use **Frozen Lake** as an example.\n",
        "\n",
        "Load the Frozen Lake environment"
      ]
    },
    {
      "metadata": {
        "id": "tUw5ImolAWFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qsiOp9bAXUH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9405beb7-50d6-47e1-9519-955fb175e3ef"
      },
      "cell_type": "code",
      "source": [
        "# make function of gym loads the specified environment\n",
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "f7cmVERTAo4s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, reset the environment."
      ]
    },
    {
      "metadata": {
        "id": "X6i1DlsqBKqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ffbae1a5-423b-4c8c-f62d-db7b046a6255"
      },
      "cell_type": "code",
      "source": [
        "s = env.reset()\n",
        "# reset the environment and returns the state state as a value\n",
        "print('Initial state is',s)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial state is 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5v6UzW4DBYxb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While performing a RL task, an agent undergoes learning through multiple episodes. As a result, at the start of each episode, the environment needs to be **reset** so that it comes to its initial situation and the agent begins from the start state."
      ]
    },
    {
      "metadata": {
        "id": "FDGERagTBqd6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After taking each action, it may be a requirement to show the status of the agent in the environment. This can be visualized by executing:"
      ]
    },
    {
      "metadata": {
        "id": "FJj1txwUBVj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "87eb877a-9768-4673-f01c-92e13a6aa270"
      },
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rpqT3GExB4dj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This output shows that this is an environment with 4x4 girds, that is, 16 states arranged in the preceding manner where S, H, F, and G represent different forms of a state where:\n",
        "* **S**: Start block\n",
        "* **F**: Frozen block\n",
        "* **H**: Block has hole\n",
        "* **G**: Goal block"
      ]
    },
    {
      "metadata": {
        "id": "Zm8pOxfpC6p_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The environment features cannot be modified directly, but this can be done by unwrapping the environment parameters with:"
      ]
    },
    {
      "metadata": {
        "id": "EkxEHu5WDFUh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = env.unwrapped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xO7SxERVDLnJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each environment is defined by the **state spaces** and **action spaces** for the agent to perform. To access the type (discrete or continuous) and size of state spaces and action spaces:"
      ]
    },
    {
      "metadata": {
        "id": "qCp4_uWyEDr4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Action Space**:"
      ]
    },
    {
      "metadata": {
        "id": "5DH91AcYDHc0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71c10148-51d1-4811-a7e7-23a165c00a92"
      },
      "cell_type": "code",
      "source": [
        "print('Type of Action Space:', env.action_space)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of Action Space: Discrete(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oIUVCdt5Dj-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98cff436-bb89-4e32-ac66-0daa4e911e86"
      },
      "cell_type": "code",
      "source": [
        "print('Size of Action Space:', env.action_space.n)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Action Space: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ScmdrMIDDvLw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`Discrete(4)` means the action space of the Frozen Lake environemnt is a discrete set of values and has four distinct actions that can be performed by the agent."
      ]
    },
    {
      "metadata": {
        "id": "rIZ5az5VEHhx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**State Space**:"
      ]
    },
    {
      "metadata": {
        "id": "Sz9feSrtEJik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c134f95b-d7eb-4bd3-92cc-870e82ffdb3a"
      },
      "cell_type": "code",
      "source": [
        "print('Type of State Space:', env.observation_space)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of State Space: Discrete(16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lsIgdRmQEONI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7645598e-534f-41cb-eead-84c5a47a9007"
      },
      "cell_type": "code",
      "source": [
        "print('Size of State Space:', env.observation_space.n)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of State Space: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N4k4N0DwET38",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`Discrete(16)` means that the observation (state) space of the Frozen\n",
        "Lake environment is a discrete set of values and has 16 different states to be explored by the\n",
        "agent."
      ]
    },
    {
      "metadata": {
        "id": "mJgQxaCuEfjN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Program an Agent Using an OpenAI Gym Environment\n",
        "This toy example environment consists of 4x4 grids representing a lake. Thus, there are 16 grid blocks, where each block can be a start block(S), frozen block(F), goal block(G), or a hole block(H). Therefore, the objective of the agent is to learn to navigate from start to goal without falling in the hole.\n",
        "\n",
        "Reset the environment and start a new environment:"
      ]
    },
    {
      "metadata": {
        "id": "qgjOZV_yFTQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "82f6ae18-f827-40e5-ef8b-28c3a19ea67a"
      },
      "cell_type": "code",
      "source": [
        "# load the environment FrozenLake-v0\n",
        "env = gym.make('FrozenLake-v0')\n",
        "env.render()\n",
        "# output the environment and position of the agent"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "702KJwOnFkRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At any given state, an agent has four actions to perform, which are up, down, left, and right.\n",
        "The reward at each step is 0 except the one leading to the goal state, then the reward would\n",
        "be 1. Start from the S state and the goal is to reach the G state without landing up in the H state in the most optimized path through the F states."
      ]
    },
    {
      "metadata": {
        "id": "o-J8EkCiGFJt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Q-Learning\n",
        "Q-learning consists of a Q-table that contains Q-values for each state-action pair.\n",
        "\n",
        "The number of rows\n",
        "in the table is equal to the number of states in the environment and the number of columns\n",
        "equals the number of actions.\n",
        "\n",
        "In this example, the number of states is 16 and the number of actions is 4, the Q-table for this environment consists of 16 rows and 4 columns."
      ]
    },
    {
      "metadata": {
        "id": "Yb4o78zjG378",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "10a35d35-be79-416f-93d7-5b3857054c6a"
      },
      "cell_type": "code",
      "source": [
        "print('Number of actions : ', env.action_space.n)\n",
        "print('Number of states : ', env.observation_space.n)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of actions :  4\n",
            "Number of states :  16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "twfn999FHGzv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Steps in Q-learning:\n",
        "1. Initialize the Q-table with zeros (eventually, updating will happen with a reward received for each action taken during learning).\n",
        "2. Updating of a Q value for a state-action pair, that is, $Q(s,a)$ is given by:\n",
        "$$Q(s,a)\\leftarrow Q(s,a)+\\alpha[r+\\gamma\\max_{a'}Q(s',a')-Q(s,a)]$$\n",
        "where\n",
        " * $s$ = current state\n",
        " * $a$ = action taken (choosing new action through epsilon-greedy approach)\n",
        " * $s'$ = resulted new state\n",
        " * $a'$ = action for the new state\n",
        " * $r$ = reward received for the action $a$\n",
        " * $\\alpha$= learning rate, that is, the rate at which the learning of the agent converges towards minimized error\n",
        " * $\\gamma$= discount factor, that is, discounts the future reward to get an\n",
        "idea of how important that future reward is with regards to the\n",
        "current reward\n",
        "3. By updating the Q-values as per the formula mentioned in step 2, the table\n",
        "converges to obtain accurate values for an action in a given state."
      ]
    },
    {
      "metadata": {
        "id": "1WhgjjWIGG3h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Epsilon-Greedy approach\n",
        "The Epsilon-Greedy is a widely used solution to the **explore-exploit** dilemma.\n",
        "\n",
        "**Exploration** is all about searching and exploring new options through experimentation and research to generate new values, while **exploitation** is all about refining existing options by repeating those options and improving their values.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "    epsilon = 0.05 or 0.1 # any small value between 0 to 1 \n",
        "    #epsilon is the probability of exploration\n",
        "    \n",
        "    p = random number between 0 and 1\n",
        "    if p <= epsilon:\n",
        "      pull a random action\n",
        "    else:\n",
        "      pull current best action\n",
        "\n",
        "Eventually, after several iterations, the best actions are obtained among all at each state\n",
        "because it gets the option to explore new random actions as well as exploit the existing\n",
        "actions and refine them."
      ]
    },
    {
      "metadata": {
        "id": "31Rkj_RTJ7xC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the FrozenLake example, the following implementation of a basic Q-learning algorithm is to make an agent learn how to navigate across this frozen lake of 16 grids, from the start to the goal without falling into the hole."
      ]
    },
    {
      "metadata": {
        "id": "jqZ-FPqvKMXN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from __future__ import print_function\n",
        "import gym\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lg9ZgDoXKY07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "b30430ab-678d-46e6-cd9b-471e32e1af24"
      },
      "cell_type": "code",
      "source": [
        "# load the environment\n",
        "env = gym.make('FrozenLake-v0')\n",
        "s = env.reset()\n",
        "print('Initial state : ', s)\n",
        "print()\n",
        "\n",
        "env.render()\n",
        "print()\n",
        "\n",
        "print(env.action_space) # number of actions\n",
        "print(env.observation_space) # number of states\n",
        "print()\n",
        "\n",
        "print('Number of actions : ', env.action_space.n)\n",
        "print('Number of states : ', env.observation_space.n)\n",
        "print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial state :  0\n",
            "\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Discrete(4)\n",
            "Discrete(16)\n",
            "\n",
            "Number of actions :  4\n",
            "Number of states :  16\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vVPJ7K38K_LQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Epsilon-Greedy\n",
        "def epsilon_greedy(Q, s, na):\n",
        "  epsilon = 0.3\n",
        "  p = np.random.uniform(low=0, high=1)\n",
        "  #print('p = ', p)\n",
        "  if p > epsilon:\n",
        "    return np.argmax(Q[s,:])\n",
        "    '''for each state consider the action having highest Q-value...'''\n",
        "  else:\n",
        "    return env.action_space.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kp2fU14wMJ-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ## Q-learning implementation\n",
        "\n",
        "# initializing Q-table with zeros\n",
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# set hyperparameters\n",
        "lr = 0.5 # learning rate\n",
        "y = 0.9 # discount factor\n",
        "eps = 100000 # total episodes being 100000\n",
        "\n",
        "\n",
        "for i in range(eps):\n",
        "  s = env.reset()\n",
        "  t = False\n",
        "  while(True):\n",
        "    a = epsilon_greedy(Q, s, env.action_space.n)\n",
        "    s_, r, t, _ = env.step(a)\n",
        "    if (r==0):\n",
        "      if t==True:\n",
        "        r = -5 # give negative rewards when holes turn up\n",
        "        Q[s_] = np.ones(env.action_space.n) * r # in terminal state Q value equals the reward\n",
        "      else:\n",
        "        r = -1 # give negative rewards to avoid long routes\n",
        "    if (r==1):\n",
        "      r = 100\n",
        "      Q[s_] = np.ones(env.action_space.n) * r # in terminal state Q value equals the reward\n",
        "    \n",
        "    Q[s,a] = Q[s,a] + lr * (r + y*np.max(Q[s_,a]) - Q[s,a])\n",
        "    s = s_\n",
        "    if (t==True):\n",
        "      break\n",
        "      \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "db_mUfIsQ4zD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "869e7c66-6bbe-42e3-f92d-d39cbe7e5868"
      },
      "cell_type": "code",
      "source": [
        "print('Q-table')\n",
        "print(Q)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q-table\n",
            "[[ -9.83778557  -3.24135919  -9.63678007 -10.        ]\n",
            " [ -9.78785909  -7.98648554  -9.58707493 -10.        ]\n",
            " [ -9.75532542  -6.00764762  -9.58265224 -10.        ]\n",
            " [ -9.73179677  -9.15491916  -9.5942822  -10.        ]\n",
            " [ -9.80030994  -5.81278331  -9.55178373  -9.91776723]\n",
            " [ -5.          -5.          -5.          -5.        ]\n",
            " [ -9.76763265  -3.00126116  -9.38367652  -9.69605172]\n",
            " [ -5.          -5.          -5.          -5.        ]\n",
            " [ -9.6958445    1.61616862  -9.41833013  -9.66906161]\n",
            " [ -9.58194434  11.99208401  -8.18834419  -9.57363767]\n",
            " [ -9.63871314  73.21798673  -9.43075185  -9.56714924]\n",
            " [ -5.          -5.          -5.          -5.        ]\n",
            " [ -5.          -5.          -5.          -5.        ]\n",
            " [ -9.55076316  84.58387031  -7.28222463  -7.389242  ]\n",
            " [ -9.61050912 146.88923396   7.32480692  -2.15727352]\n",
            " [100.         100.         100.         100.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KWwvkoxIRBka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1075
        },
        "outputId": "9e03067f-2daa-4499-d044-00ae354e0177"
      },
      "cell_type": "code",
      "source": [
        "print('Output after learning')\n",
        "print()\n",
        "# learning ends with the end of the above loop of several episodes above\n",
        "s = env.reset()\n",
        "env.render()\n",
        "while(True):\n",
        "  a = np.argmax(Q[s])\n",
        "  s_, r, t, _ = env.step(a)\n",
        "  print('====================')\n",
        "  env.render()\n",
        "  s = s_\n",
        "  if (t==True):\n",
        "    break"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output after learning\n",
            "\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "====================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "====================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "====================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "====================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "====================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "====================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "====================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "====================\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "====================\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5bvXKWlXGKXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Q-Network for Real-World Applications\n",
        "Q-table is a good option for the real world problem since the states become infinite. This is where neural network acts a function approximator, which is trained over data of different state information and their corresponding Q-values for all action, thereby, they are abl to predict Q-values for any new state information input.\n",
        "\n",
        "The neural network used to predict Q-values instead of using a Q-table is called **Q-network**.\n",
        "\n",
        "In the toy example, `FrozenLake-v0` environment, a single neural network taking state information as input is used, where the state information is represented as a **one-hot encoded** vector of the **1 x number of states** shape (here, 1 x 16) and outputs a vecotr of the **1 x number of actions** shape (here, 1 x 4).\n",
        "\n",
        "For example, consider  there are 16 states numbered from state 0 to state 15, then state number 4 will be represented in one-hot encoded vector as\n",
        "\n",
        "`input_state = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]`\n",
        "\n",
        "Advantage of Q-network over Q-table is to add more hidden layers and different activation functions. The Q-values in a Q-network are updated by minimizing the loss through backpropagation. The loss functions is given by:\n",
        "$$\\mathrm{Loss}=\\sum(Q_{target}-Q_{predicted})^2$$\n",
        "and\n",
        "$$Q(s,a)_{target}=r+\\gamma\\max_{a'}Q(s',a')$$\n",
        "\n",
        "The following code is to implement a basic Q-Network to make an agent learn to navigate across this frozen lake of 16 grids from the\n",
        "start to the goal without falling into the hole:"
      ]
    },
    {
      "metadata": {
        "id": "sRKI5SVDUm1W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aAFP4BkMUs6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8beeb404-1375-4df0-8094-ea008ed23033"
      },
      "cell_type": "code",
      "source": [
        "# load the environment\n",
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "oVEyDfzyU4qN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ## Q-Network Implementation\n",
        "\n",
        "# ### Create neural network\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# tensors for inputs, weights, biases, Qtarget\n",
        "inputs = tf.placeholder(shape=[None, env.observation_space.n], \n",
        "                        dtype=tf.float32)\n",
        "W = tf.get_variable(name='W', \n",
        "                    dtype=tf.float32,\n",
        "                    shape=[env.observation_space.n, env.action_space.n],\n",
        "                    initializer=tf.contrib.layers.xavier_initializer())\n",
        "b = tf.Variable(tf.zeros(shape=[env.action_space.n]),\n",
        "                dtype=tf.float32)\n",
        "\n",
        "qpred = tf.add(tf.matmul(inputs, W), b)\n",
        "apred = tf.argmax(qpred, 1)\n",
        "\n",
        "qtar = tf.placeholder(shape=[1, env.action_space.n], dtype=tf.float32)\n",
        "loss = tf.reduce_sum(tf.square(qtar - qpred))\n",
        "\n",
        "train = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "minimizer = train.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ohm_KtaaWOKz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6621
        },
        "outputId": "b68bb1e0-2042-4d50-db01-ce6cbd48efb3"
      },
      "cell_type": "code",
      "source": [
        "# ### Train neural network\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# learning parameters\n",
        "y = 0.5 # discount factor\n",
        "e = 0.3 # epsilon value for epsilon-greedy task\n",
        "episodes = 10000 # total number of episodes\n",
        "\n",
        "# list to capture total steps and rewards per episodes\n",
        "slist = []\n",
        "rlist = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for i in range(episodes):\n",
        "    s = env.reset() # reset the environment at the start of each episode\n",
        "    r_total = 0 # calculate the sum of rewards in the current episode\n",
        "    while(True):\n",
        "      # run the Q-Network created above\n",
        "      a_pred, q_pred = sess.run([apred, qpred], \n",
        "                                feed_dict={inputs:np.identity(env.observation_space.n)[s:s+1]})\n",
        "      '''a_pred is the action prediction by the neural network\n",
        "         q)pred contains q_values of the actions at current state s'''\n",
        "      \n",
        "      if np.random.uniform(low=0, high=1) < e:\n",
        "        a_pred[0] = env.action_space.sample()\n",
        "        '''explore different action by randomly assigning them as the next action'''\n",
        "      \n",
        "      s_, r, t, _ = env.step(a_pred[0]) \n",
        "      '''action taken and new state s_ is encountered with a feedback reward r'''\n",
        "      \n",
        "      if r==0:\n",
        "        if t==True:\n",
        "          r = -5 # if hole, make the reward more negative\n",
        "        else:\n",
        "          r = -1 # if block is fine/frozen then give slight negative reward to optimize the path\n",
        "      if r==1:\n",
        "          r = 5 # good positive GOAT state reward\n",
        "          \n",
        "      q_pred_new = sess.run(qpred,\n",
        "                            feed_dict={inputs:np.identity(env.observation_space.n)[s_:s_+1]})\n",
        "      '''q_pred_new contains q_values of the actions at the new state'''\n",
        "      \n",
        "      # update the Q-target value for action taken\n",
        "      targetQ = q_pred\n",
        "      max_qpredn = np.max(q_pred_new)\n",
        "      targetQ[0, a_pred[0]] = r + y*max_qpredn\n",
        "      '''This gives the targetQ'''\n",
        "      \n",
        "      # train the neural network to minimize the loss\n",
        "      _ = sess.run(minimizer,\n",
        "                   feed_dict={inputs:np.identity(env.observation_space.n)[s:s+1],\n",
        "                              qtar:targetQ})\n",
        "      r_total += r\n",
        "      \n",
        "      s = s_\n",
        "      if t==True:\n",
        "        break\n",
        "      \n",
        "    if i%200==0:\n",
        "      print('Training #{} is completed.'.format(i))\n",
        "        \n",
        "  '''learning ends with the end of the loop of several episodes above\n",
        "     Check how much the agent has learned'''\n",
        "  print('Output after learning')\n",
        "  print()\n",
        "  s = env.reset()\n",
        "  env.render()\n",
        "  while(True):\n",
        "    a = sess.run(apred,\n",
        "                 feed_dict={inputs:np.identity(env.observation_space.n)[s:s+1]})\n",
        "    s_, r, t, _ = env.step(a[0])\n",
        "    print('================')\n",
        "    env.render()\n",
        "    s = s_\n",
        "    if t==True:\n",
        "      break"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training #0 is completed.\n",
            "Training #200 is completed.\n",
            "Training #400 is completed.\n",
            "Training #600 is completed.\n",
            "Training #800 is completed.\n",
            "Training #1000 is completed.\n",
            "Training #1200 is completed.\n",
            "Training #1400 is completed.\n",
            "Training #1600 is completed.\n",
            "Training #1800 is completed.\n",
            "Training #2000 is completed.\n",
            "Training #2200 is completed.\n",
            "Training #2400 is completed.\n",
            "Training #2600 is completed.\n",
            "Training #2800 is completed.\n",
            "Training #3000 is completed.\n",
            "Training #3200 is completed.\n",
            "Training #3400 is completed.\n",
            "Training #3600 is completed.\n",
            "Training #3800 is completed.\n",
            "Training #4000 is completed.\n",
            "Training #4200 is completed.\n",
            "Training #4400 is completed.\n",
            "Training #4600 is completed.\n",
            "Training #4800 is completed.\n",
            "Training #5000 is completed.\n",
            "Training #5200 is completed.\n",
            "Training #5400 is completed.\n",
            "Training #5600 is completed.\n",
            "Training #5800 is completed.\n",
            "Training #6000 is completed.\n",
            "Training #6200 is completed.\n",
            "Training #6400 is completed.\n",
            "Training #6600 is completed.\n",
            "Training #6800 is completed.\n",
            "Training #7000 is completed.\n",
            "Training #7200 is completed.\n",
            "Training #7400 is completed.\n",
            "Training #7600 is completed.\n",
            "Training #7800 is completed.\n",
            "Training #8000 is completed.\n",
            "Training #8200 is completed.\n",
            "Training #8400 is completed.\n",
            "Training #8600 is completed.\n",
            "Training #8800 is completed.\n",
            "Training #9000 is completed.\n",
            "Training #9200 is completed.\n",
            "Training #9400 is completed.\n",
            "Training #9600 is completed.\n",
            "Training #9800 is completed.\n",
            "Output after learning\n",
            "\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "================\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "================\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "================\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "================\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "================\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "================\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5XlZHof6jhmv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is a cost of stability associated with both Q-learning and Q-networks. There will be\n",
        "cases when with the given set of hyperparameters of the Q-values are not converge, but\n",
        "with the same hyperparameters, sometimes converging is witnessed.\n",
        "\n",
        "This is because of the\n",
        "instability of these learning approaches. In order to tackle this, a better initial policy should\n",
        "be defined (here, the maximum Q-value of a given state) if the state space is small.\n",
        "Moreover, hyperparameters, especially learning rate, discount factors, and epsilon value,\n",
        "play an important role. Therefore, these values must be initialized properly.\n",
        "\n",
        "Q-networks provide more flexibility compared to Q-learning, owing to increasing state\n",
        "spaces. A deep neural network in a Q-network might lead to better learning and\n",
        "performance."
      ]
    }
  ]
}