{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch3_MDP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/reinforcement_learning_with_tensorflow/blob/master/ch3_MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Q7Iw0KRD8oHI",
        "colab_type": "code",
        "outputId": "46a9395d-b7b9-44c7-91ac-f96bc34c2b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aPhFmXP_8rsF",
        "colab_type": "code",
        "outputId": "8a8f18b9-78e8-4500-e71e-97fe713a5b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My' 'Drive/Colab' 'Notebooks/Reinforcement_Learning_with_TensorFlow/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Reinforcement_Learning_with_TensorFlow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PZeDqyB19IN9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Markov Decision Processes\n",
        "The Markov decision process (MDP) is a reinforcement learning approach in a gridworld environment containing sets of states, actions, and rewards, following the Markov property to obtain an optimal policy.\n",
        "\n",
        "MDP is defined as the collection of:\n",
        "* **States**: $S$\n",
        "* **Actions**: $A(s), A$\n",
        "* **Transition model**: $T(s,a,s')\\sim P(s'|s,a)$\n",
        "* **Rewards**: $R(s), R(s,a), R(s,a,s')$\n",
        "* **Policy**: $\\pi(s)\\rightarrow a_{\\pi^*}$ is the optimal policy\n",
        "\n",
        "The MDP environment is fully observable, which means whatever observation the agent makes at any point in time is enough to make an optimal decision.\n",
        "\n",
        "For partially observable environment, the agent needs a memory to store the past observations to make the best possible decisions."
      ]
    },
    {
      "metadata": {
        "id": "eLor5Ue0-ne1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Markov property\n",
        "The present information at time $t$ matters in order to know the information of near future at time $t+1$. \n",
        "\n",
        "*First Order* of Markov: $x_t$ depends only on $x_{t-1}$.\n",
        "$$P(x_t|x_{t-1},x_{t-2},...,x_t)=P(x_t|x_{t-1})$$\n",
        "\n",
        "*Second Order* of Markov: $x_t$ depends only on $x_{t-1}$ and $x_{t-2}$.\n",
        "$$P(x_t|x_{t-1},x_{t-2},...,x_t)=P(x_t|x_{t-1}, x_{t-2})$$.\n",
        "\n",
        "Most of the RL algorithms are based on the first order of Markov property."
      ]
    },
    {
      "metadata": {
        "id": "nuvQG6g4CLbA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##$S$ State Set\n",
        "is a set of different states, represented as $s$ constituting the envrionment. States are the feature representation of the data obtained from the environment. State spaces can be discrete or continuous.\n",
        "\n",
        "Consider a gridworld having 12 discrete states:\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch3/gridworld.PNG?raw=true)\n",
        "\n",
        "The states can be represented as 1, 2,..., 12 or by coordinates, (1, 1), (1, 2), ...., (3, 4)\n"
      ]
    },
    {
      "metadata": {
        "id": "NgAkGIvoDW5X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Actions\n",
        "are the choices an agent can perform or execute in a particular state. actions can also be discrete or continuous. For the gridworld shown above, it has 12 discrete states and 4 discrete actions which are *UP*, *DOWN*, *RIGHT*, and *LEFT*.\n",
        "\n",
        "The action space can be represented as $a\\in A$, where $A=\\{UP, DOWN, RIGHT, LEFT\\}$. It can also be represented as a function of state, $a=A(s)$, where depending on the state function, it decides which action is possible."
      ]
    },
    {
      "metadata": {
        "id": "N0sL8ldtEnDE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Transition model\n",
        "is defined as a function of three variables $T(s,a,s')$, which consists of the *current state* $s$, *action* $a$, and the *new state* $s'$. The transition model also defines the rules to play game in the envirnoment and gives probability $P(s'|s,a)$, the probability of landing up in the new $s'$ state given that the agent takes an action, $a$, in given state, $s$.\n",
        "\n",
        "Still consider the gridworld example,\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch3/gridworld.PNG?raw=true)\n",
        "\n",
        "This environment can be considered as*determined* or *stochastic*:\n",
        "* **Determined environment**: The certain action will be certainly performed with probability 1.\n",
        "* **Stochastic environment**: The same action will ben taken with a probability less than 1 and there are other probable actions to select."
      ]
    },
    {
      "metadata": {
        "id": "yQ2_zQg8G1PB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Rewards\n",
        "quantifies the usefulness of entering into a state. The forms to represent the reward are: $R(s), R(s,a), R(s,a,s')$, equivalently.\n",
        "\n",
        "Two approaches to reward the agent for when taking a certain action:\n",
        "* **Credit assignment problem**: look at the past and check which actions led to the present reward. Find which action gets the credit\n",
        "* **Delayed rewards**: check which action to take in the present state will lead to potential rewards."
      ]
    },
    {
      "metadata": {
        "id": "aIsihcseNvOT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Policy\n",
        "is the solution to the MDP problem.\n",
        "$$\\mathrm{Policy}:\\pi(s)\\rightarrow a$$\n",
        "The policy is a function taking the state as an input and outputs the action to be taken, so the policy is a command that the agent has to obey.\n",
        "\n",
        "$\\pi^*$ is the optimal policy which maximizes the expect reward."
      ]
    },
    {
      "metadata": {
        "id": "V8IRklgdOT5M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Sequence of rewards\n",
        "is used to find the optimal policy for MDP problem, but there are assumptions for a sequence of rewards applied on the concept of delayed rewards.\n",
        "\n",
        "###Assumption 1: The inifinite horizons\n",
        "The infinite amount of time steps to reach goal state from start state. Thus, $\\pi(s)\\rightarrow a$ so the policy function does not take the remaining time steps. If it had been a finite horizon, $\\pi(s,t)\\rightarrow a$, where $t$ is the time steps left to get the task done.\n",
        "\n",
        "###Assumption 2: Utility of sequences\n",
        "The utility of sequences refers to the overall reward received when the agent goes through the sequences of states, defined as $U(s_0, s_1, s_2,...)$, where $s_0, s_1, s_2,...$ represents the sequence of states.\n",
        "\n",
        "This assumption is that if there are two utilities, $U(s_0, s_1, s_2,....)$ and $U(s_0, s'_1, s'_2,....)$ with the same start state, and \n",
        "$$U(s_0, s_1, s_2,....)>U(s_0, s'_1, s'_2,....)$$\n",
        "then,\n",
        "$$U(s_1, s_2,....)>U(s'_1, s'_2,....)$$\n",
        "This called the stationary of preferences.\n",
        "\n",
        "The following equation implements the concept of delayed rewards and satisfies the stationary of preferences:\n",
        "$$U(s_0,s_1,s_2,...)=\\sum\\limits_{t=0}^\\infty \\gamma^t R(s_t)$$\n",
        "where $0\\leq\\gamma\\leq 1$\n",
        "\n",
        "This summation of all the rewards $R(s_t)$ has a upper bound,\n",
        "$$\\sum\\limits_{t=0}^\\infty \\gamma^t R(s_t) \\leq \\frac{R_\\max}{1-\\gamma}$$\n",
        "where $R_\\max$ is the maximum reward value"
      ]
    },
    {
      "metadata": {
        "id": "J02cmWltVlcR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Bellman equations\n",
        "$$\\pi^*=\\arg\\max_\\pi E[\\sum\\limits_{t=0}^\\infty \\gamma^t R(s_t)|\\pi]$$\n",
        "where $E[\\sum\\limits_{t=0}^\\infty \\gamma^t R(s_t)|\\pi]$ means the expected value of the rewards obtained from the sequence of states agent observes if it follows the $\\pi$ policy. In addition, $\\arg\\max_\\pi$ outputs the $\\pi$ policy that has the highest expected reward.\n",
        "\n",
        "**Utility of the policy of a state**, $U^\\pi(s)$ at the $s$ state, given a $\\pi$ policy, is the expected rewards from that state onward:\n",
        "$$U^\\pi(s)=E[\\sum\\limits_{t=0}^\\infty \\gamma^t R(s_t)|\\pi, s_0=s]$$\n",
        "\n",
        "Immediate reward of the state: $R(s)$\n",
        "\n",
        "Optimal policy: policy that maximizes the expected utility\n",
        "$$\\pi^*=\\arg\\max_a\\sum\\limits_{s'}T(s,a,s')U(s')$$\n",
        "where $T(s,a,s')$ is the transition probability, $P(s'|s,a)$ and $U(s')$ is the utility of the new landing state after the $a$ action is taken on the $s$ state.\n",
        "\n",
        "The utility of the $s$ state is given by the Bellman equation:\n",
        "$$U(s)=R(s)+\\gamma\\max_a\\sum\\limits_{s'}T(s,a,s')U(s')$$\n",
        "where\n",
        "* $R(s)$ is the immediate reward\n",
        "* $\\max_a\\sum\\limits_{s'}T(s,a,s')U(s')$ is the reward from future, discounted utilities of the $s$ state where the agent can reach from the given $s$ state if the action $a$ is taken."
      ]
    },
    {
      "metadata": {
        "id": "Ejmra-yreA5M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Solve the Bellman equation to find policies\n",
        "Suppose $n$ states are given in the environment and the Bellman equation, thus, there are $n$ equations and $n$ unknowns.\n",
        "To solve this system of equations,\n",
        "1. Start with an arbitrary utility\n",
        "2. Update the utilities based on the neighborhood until convergence, that is, update\n",
        "the utility of the state using the Bellman equation based on the utilities of the\n",
        "landing states from the given state\n",
        "\n",
        "####Value iteration\n",
        "iterates multiple times to convergence towards the true value of the states.\n",
        "####Policy iteration\n",
        "iterates over the policy and updates the policy itself instead of value until the policy converges to the optimum.\n",
        "\n",
        "Process of policy iteration:\n",
        "* Start with a random policy, $\\pi_0$\n",
        "* For the given $\\pi_t$ policy at iteration step $t$, calculate $U_t=U_t^\\pi$ using:\n",
        "$$U_t(s)=R(s)+\\gamma\\max_a\\sum\\limits_{s'}T(s,a,s')U_{t-1}(s')$$\n",
        "* Improve the $\\pi_{t+1}$ policy by\n",
        "$$\\pi_{t+1}=\\arg\\max_a\\sum\\limits_{s'}T(s,a,s')U_t(s')$$"
      ]
    },
    {
      "metadata": {
        "id": "mGf9SccejuL5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Partially Observable Markov Decision Processes\n",
        "In MDP, the observable quantities are action, set $A$, the state, set $S$, transition model, $T$,\n",
        "and rewards, set $R$.\n",
        "\n",
        "In **partially observable MDP**, a.k.a, **POMDP**, some of the above quantities are not directly observable to the agent.\n",
        "\n",
        "In POMDP, there is an observation set, $Z$, containing different observable states and a observation function, $O$, which takes the $s$ state and the $z$ observation as inputs and outputs the probability of seeing that $z$ observation in the $s$ state.\n",
        "\n",
        "In summary,\n",
        "* **MDP**: $\\{S,A,T,R\\}$\n",
        "* **POMDP**: $\\{S,A,Z,T,R,O\\}$\n",
        "\n",
        "where $S,A,T,R$ are the same.\n",
        "\n",
        "For a POMDP to be a true MDP, the following conditions must be satisfied:\n",
        "\n",
        "* $Z=S$, that is, fully observe all states \\\\\n",
        "* $\n",
        "O(s,z) = \\left\\{\n",
        "        \\begin{array}{ll}\n",
        "            1 & \\quad s=z \\\\\n",
        "            0 & \\quad \\mathrm{otherwise}\n",
        "        \\end{array}\n",
        "    \\right.\n",
        "$\n",
        "\n",
        "POMDP are hugely intractable to solve optimally."
      ]
    },
    {
      "metadata": {
        "id": "DyP491vwm0N-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##State estimation\n",
        "Expansion of the stat spaces helps convert the POMDP into an MDP where $Z$ contains fully observable state space.\n",
        "\n",
        "The concept of the **belief state**, $b(s)$, is introduced here, which is the state that the decision maker uses in the context of a POMDP. This belief state, $b(s)$, gives the probability of the agent being in the $s$ state. Thus, the belief state, $b$, is a vector representing the probability distribution over all states and it get updated once an action is taken.\n",
        "\n",
        "For example, \\\\\n",
        "there is a belief state, $b$, the agent takes an action, $a$, and some observations, $z$, received, then this will form a new belief state. Therefore, the new belief state, $b'(s')$,\n",
        "$$b'(s')=\\mathrm{probability\\,of\\, being\\,in}\\,s\\,\\mathrm{state\\,given\\,after}\\,b,a,z,\\,\\mathrm{that\\,is,}\\,p(s'|b,a,z)$$\n",
        " \\\\\n",
        "$$p(s'|s,a,z)=\\frac{p(z|b,a,s')}{p(z|b,a)}\\sum\\limits_{s}b(s)\\cdot p(s'|s,b,a)$$\n",
        "where, $p(z|b,a,s')=O(s',z)$ and $p(s'|s,b,a)=T(s,a,s')$. Therefore,\n",
        "$$p(s'|s,a,z)=\\frac{O(s',z)}{p(z|b,a)}\\sum\\limits_{s}b(s)\\cdot T(s,a,s')$$"
      ]
    },
    {
      "metadata": {
        "id": "sUz39Sf1h3vp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Value iteration in POMDPs\n",
        "Value iteration on an infinite state space obtained from a belief MDP:\n",
        "* $V_0(b)=0$ at $t=0$\n",
        "* $V_t(b)=\\max_{a}[R(b,a)+\\gamma\\sum\\limits_{z}P(z|b,a)\\cdot V_{t-1}(b')]$ at $t>0$, where $b'$ is $b'(s')=p(s'|b,a,z)$, which is the state estimation for $(b,a,z)$ and $R(b,a)$ is the expected reward over a belief state:\n",
        "$$R(b,a)=\\sum\\limits_{s}p(s)\\cdot R(s,a)=\\sum\\limits_{s}b(s)\\cdot R(s,a)$$\n",
        "where, \\\\\n",
        "$p(s)=$ probability of the $s$ state \\\\\n",
        "$R(s,a)=$ reward in that state \\\\\n",
        "$\\sum\\limits_{s}b(s)\\cdot R(s,a)=$ expected reward over a belief state"
      ]
    },
    {
      "metadata": {
        "id": "Kl39CwlhjfAM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train `FrozenLake-v0` using MDP"
      ]
    },
    {
      "metadata": {
        "id": "hcEzMu7KkUyV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dependent libraries\n",
        "from __future__ import print_function\n",
        "import gym\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s50Kn78ekb-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c6fae799-c9af-440f-c431-fb118e7bfb84"
      },
      "cell_type": "code",
      "source": [
        "# load environment\n",
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vQ3PVZGlklq1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "833b2845-6eff-4e0e-8cd0-5d88f2062bd5"
      },
      "cell_type": "code",
      "source": [
        "s = env.reset()\n",
        "print(s)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mOGX8zOTkoiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "6d474c9a-81c6-4313-f9e0-4bfa2ae67a21"
      },
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zt-q8VFZkqwx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6fa19f9e-0ad9-470d-f17d-ca14a76ba99e"
      },
      "cell_type": "code",
      "source": [
        "print(env.action_space) # number of actions\n",
        "print(env.observation_space) # number of states"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n",
            "Discrete(16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jlwnWaUAkz5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8f14b4bc-c7ff-488b-e17c-1cd6b1703f9a"
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of actions : \",env.action_space.n)\n",
        "print(\"Number of states : \",env.observation_space.n)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of actions :  4\n",
            "Number of states :  16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_Y9hC2N5k3he",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "34805e0d-07a1-428f-950c-ba6adf3fd334"
      },
      "cell_type": "code",
      "source": [
        "# ## Value Iteration Implementation\n",
        "\n",
        "# initialize utilities of all states with zeros\n",
        "U = np.zeros([env.observation_space.n])\n",
        "\n",
        "# terminal states have utility values equal to their reward\n",
        "U[15] = 1 # goal states\n",
        "U[[5,7,11,12]] = -1 # hole states\n",
        "termS = [5,7,11,12,15] # terminal states\n",
        "\n",
        "# set hyperparameters\n",
        "y = 0.8 # discounted factor\n",
        "eps = 1e-3 # threshold if the learning difference i.e. prev_u - U goes below\n",
        "           # this value break the learning\n",
        "  \n",
        "i=0\n",
        "while(True):\n",
        "  i += 1\n",
        "  prev_u = np.copy(U)\n",
        "  for s in range(env.observation_space.n):\n",
        "    q_sa = [sum([p*(r + y*prev_u[s_]) for p, s_, r, _ in env.env.P[s][a]]) for a in range(env.action_space.n)]\n",
        "    if s not in termS:\n",
        "      U[s] = np.max(q_sa)\n",
        "      \n",
        "  if (np.sum(np.fabs(prev_u - U))<=eps):\n",
        "    print('Value-iteration converged at iteration# {}'.format(i+1))\n",
        "    break\n",
        "    \n",
        "print(\"After learning completion printing the utilities for each states below from state ids 0-15\")\n",
        "print()\n",
        "print(U[:4])\n",
        "print(U[4:8])\n",
        "print(U[8:12])\n",
        "print(U[12:16])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value-iteration converged at iteration# 25\n",
            "After learning completion printing the utilities for each states below from state ids 0-15\n",
            "\n",
            "[0.023482   0.00999637 0.00437564 0.0023448 ]\n",
            "[ 0.0415207  -1.         -0.19524141 -1.        ]\n",
            "[ 0.09109598  0.20932556  0.26362693 -1.        ]\n",
            "[-1.          0.43048408  0.97468581  1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kK_5LdK4oSLG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What does this output mean? \\\\\n",
        "The state representation is interpreted as following: \\\\\n",
        "0 1 2 3 \\\\\n",
        "4 5 6 7 \\\\\n",
        "8 9 10 11 \\\\\n",
        "12 13 14 15 \\\\\n",
        "The starting state is $s=0$, then $U(s=0)=0.023482$, next there are 4 actions available: *UP, DOWN, LEFT, RIGHT*, \\\\\n",
        "At, $s=0$, \n",
        "* action UP is taken, the $s\\_new = 0$, therefore, $u[s\\_new] =0.023482$\n",
        "* action DOWN is taken, the $s\\_new = 4$, therefore, $u[s\\_new] = 0.0415207$\n",
        "* action LEFT is taken, the $s\\_new = 0$, therefore, $u[s\\_new] =0.023482$\n",
        "* action RIGHT is taken, the $s\\_new = 1$, therefore, $u[s\\_new] = 0.00999637$\n",
        "\n",
        "Thus, max is $u(s\\_new=4)-0.0415207$, therefore, the action taken is DOWN and $s\\_new=4$.\n",
        "\n",
        "Now at $=4$,\n",
        "... [continue the same process]\n",
        "\n",
        "Therefore, the final policy contains DOWN, DOWN, RIGHT, DOWN, RIGHT, and RIGHT to reach from $s=0$ (start state) to $s=15$ (goal state) by avoiding hole state (5, 7, 11, 12)"
      ]
    }
  ]
}