{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch4_PG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/reinforcement_learning_with_tensorflow/blob/master/ch4_PG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "HLQa5fByH4b0",
        "colab_type": "code",
        "outputId": "c935e315-57f8-41e8-f9e7-8866dd0c8cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gew8PZDyH6cJ",
        "colab_type": "code",
        "outputId": "ac2df9db-7352-4bd1-a270-3fefc013d3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My' 'Drive/Colab' 'Notebooks/Reinforcement_Learning_with_TensorFlow/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Reinforcement_Learning_with_TensorFlow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sACNvz_hIIgo",
        "colab_type": "code",
        "outputId": "34dbf1ed-a736-42d5-a65a-55931bd4c2bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "cell_type": "code",
      "source": [
        "#installing dependencies\n",
        "!apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip3 -q install gym\n",
        "!pip3 install piglet\n",
        "!pip3 -q install pyglet\n",
        "!pip3 -q install pyopengl\n",
        "!pip3 -q install pyvirtualdisplay\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Unable to locate package libcusparse8.0\n",
            "E: Couldn't find any package by glob 'libcusparse8.0'\n",
            "E: Couldn't find any package by regex 'libcusparse8.0'\n",
            "E: Unable to locate package libnvrtc8.0\n",
            "E: Couldn't find any package by glob 'libnvrtc8.0'\n",
            "E: Couldn't find any package by regex 'libnvrtc8.0'\n",
            "Collecting piglet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/f6/ef278239ebe525466ea51a7dd9d6d3211d197ac4b4abc76e17cdd419f69c/piglet-0.4.4.tar.gz (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 3.7MB/s \n",
            "\u001b[?25hCollecting Parsley (from piglet)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet) (18.2.0)\n",
            "Collecting astunparse (from piglet)\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/37/5dd0dd89b87bb5f0f32a7e775458412c52d78f230ab8d0c65df6aabc4479/astunparse-1.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (0.33.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (1.11.0)\n",
            "Building wheels for collected packages: piglet\n",
            "  Building wheel for piglet (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c7/59/a5/5bd1a35a4a4596714c4c7925a1751e7b1580b6ced363fd7969\n",
            "Successfully built piglet\n",
            "Installing collected packages: Parsley, astunparse, piglet\n",
            "Successfully installed Parsley-1.3 astunparse-1.6.2 piglet-0.4.4\n",
            "  Building wheel for pyvirtualdisplay (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for EasyProcess (setup.py) ... \u001b[?25ldone\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RQouZFC2wtR0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Review:\n",
        "Value-based approach: (solved by dynamic programming)\n",
        "* Value iteration\n",
        "* Policy iteration\n",
        "* Q-learning\n",
        "\n",
        "Policy optimization approach:\n",
        "* Policy gradients\n",
        "* Actor-critic algorithms\n",
        "\n",
        "As per the dynamic programming method, there are a set of self-consistent equations to\n",
        "satisfy the $Q$ and $V$ values. Policy optimization is different, where policy learning happens\n",
        "directly. Remind that the value function can be derived from the following diagram:\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch4/value_func.PNG?raw=true)\n",
        "\n",
        "Value-based methods learn the value function and an implicit policy can be derived, whereas no value function with policy-based methods can be learned and the policy is learnt directly. \n",
        "\n",
        "The actor-critic method is to learn both the value function and the policy. The network which learns the value function acts as a critic to the policy network which is the actor."
      ]
    },
    {
      "metadata": {
        "id": "SKtIOM1Py48L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Policy Optimization Method\n",
        "The objective of the policy optimization method is to find the stochastic policy $\\pi_{\\theta}$ that is a distribution of actions for a given state that maximizes the expected sum of rewards. This method aims to find the optimal policy directly. The neural network can be applied here to take the state information as input and output the distribution of possible acitons.\n",
        "\n",
        "Policy optimization:\n",
        "* **weight parameter** of the neural network is the parameter of the control policy, defined by $\\theta$ vector. Thus, the optimized $\\theta$ is the best policy. The optimal policy has the maximum overall reward. The expected sum of rewards is defined as $\\max_{\\theta}E[\\sum\\limits_{t=0}^H \\gamma^t R(s)|\\pi_{\\theta}]$ \\\\\n",
        "where $H$ is the time step at horizon. If the start time step $t=0$, then the total time steps are $H+1$\n",
        "* **stochastic policy** smooths out the policy optimization problems, unlike the deterministic policy in a grid-world environment where changes owing to change in action is not smooth. \\\\\n",
        "In a stochastic policy, $\\pi_{\\theta}(a|s)$ gives the probability of action $a$ for a given state $s$. Thus, $\\pi_{\\theta}$ gives the probabilistic distribution of actions for a given state."
      ]
    },
    {
      "metadata": {
        "id": "F3k04TWc8mwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Policy Objective Functions\n",
        "The objective in a policy method is to find the best parameter vector given policy $\\pi_{\\theta}(s,a)$ with parameter vector $\\theta$. To evaluate the quanlity of the policy $\\pi_{\\theta}(s,a)$ for different values of the parameter vector $\\theta$, the objective function $J(\\theta)$ is introduced.\n",
        "\n",
        "There are several methods to measure this quality of a policy $\\pi_{\\theta}(s,a)$:\n",
        "* For *episodic environment*, $J(\\theta)$ is the value function of the start state $V^{\\pi_{\\theta}}(s_1)$ if it starts from any state $s_1$. Thus, the *value function* is the *expected sum of reward* from that state onwards,\n",
        "$$J(\\theta)=V^{\\pi_{\\theta}}(s)$$\n",
        "* For *continuing environment*, $J(\\theta)$ is the average value function of the states. Thus, $J(\\theta)$ can be the summation of the probability of being in any state $s$ which is $d^{\\pi_{\\theta}}(s)$ times the value of that state, the expected reward from that state onward:\n",
        "$$J(\\theta)=\\sum\\limits_s d^{\\pi_{\\theta}}(s)V^{\\pi_{\\theta}}(s)$$\n",
        "* for *continuing environment*, $J(\\theta)$ can be the average reward per time step which is the summation of the probability of being in any state $s$ which is $d^{\\pi_{\\theta}}(s)$ times the expected reward over different actions for that state, $E[R^a_s]=\\sum\\limits_a \\pi_{\\theta}(s,a)R^a_s$:\n",
        "$$J(\\theta)=\\sum\\limits_s d^{\\pi_{\\theta}}(s)\\sum\\limits_a \\pi_{\\theta}(s,a)R^a_s$$\n",
        "where, $R^a_s$ is the reward at state $s$ for taking aciton $a$.\n",
        "\n",
        "The gradient-based optimization method is applied here to get greater efficiency. Since $J(\\theta)$ measures the quality of a policy, the gradient ascend method is applied to find the maximum with respect to parameter $\\theta$,\n",
        "$$\\theta^*\\leftarrow\\theta+\\alpha\\nabla_{\\theta}J(\\theta)$$\n",
        "where $\\nabla_{\\theta}J(\\theta)$ is the policy gradient and $\\alpha$ is the learning rate, called **step size parameter**.\n",
        "\n",
        "The policy gradient can be written as:\n",
        "$$\\nabla_{\\theta}J(\\theta)=\\begin{bmatrix}\n",
        "                                                       \\frac{\\nabla J(\\theta)}{\\nabla\\theta_1} \\\\\n",
        "                                                       \\frac{\\nabla J(\\theta)}{\\nabla\\theta_2} \\\\\n",
        "                                                       ... \\\\\n",
        "                                                       \\frac{\\nabla J(\\theta)}{\\nabla\\theta_n}\n",
        "                                                 \\end{bmatrix}$$"
      ]
    },
    {
      "metadata": {
        "id": "CM-yl8QtESU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Policy Gradient Theorem \n",
        "Given differentiable non-zero policy $\\pi_{\\theta}(s,a)$, the gradient of this policy is $\\nabla_{\\theta}\\pi_{\\theta}(s,a)$:\n",
        "$$\\nabla_{\\theta}\\pi_{\\theta}(s,a)=\\pi_{\\theta}(s,a)\\frac{\\nabla_{\\theta}\\pi_{\\theta}(s,a)}{\\pi_{\\theta}(s,a)} =_{\\theta}\\pi_{\\theta}(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)$$\n",
        "where $\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)$ is called the score function.\n",
        "\n",
        "Consider one-step MDP,\n",
        "* Starting state is $s$ whose probability of occurring is $d^{\\pi_{\\theta}}(s)$\n",
        "* Terminal state happens after one timestep with reward $r=R^a_s$\n",
        "\n",
        "For a continuing environment, $J(\\theta)=\\sum\\limits_s d^{\\pi_{\\theta}}(s)\\sum\\limits_a \\pi_{\\theta}(s,a)R^a_s$. Thus, the policy gradient is given as:\n",
        "$$\\nabla_{\\theta}J(\\theta)=\\sum\\limits_s d^{\\pi_{\\theta}}(s)\\sum\\limits_a \\nabla_{\\theta}\\pi_{\\theta}(s,a)R^a_s \\\\\n",
        "=\\sum\\limits_s d^{\\pi_{\\theta}}(s)\\sum\\limits_a \\pi_{\\theta}(s,a)\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a) R^a_s \\\\\n",
        "=E_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)r]$$\n",
        "\n",
        "Similarly, consider multi-step MDP, replace instantaneous reward $r$ by the state-action $Q$ value function $Q^{\\pi}(s,a)$. This is the **policy gradient theorem**. Thus, for any such policy objective function and any differentiable $\\pi_{\\theta}(s,a)$, the policy gradient is given by:\n",
        "$$\\nabla_{\\theta}J(\\theta)=E_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)Q^{\\pi_{\\theta}}(s,a)]$$"
      ]
    },
    {
      "metadata": {
        "id": "gD-tQIQDMQtG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Temporal Difference Rule\n",
        "Definition: **Temporal difference (TD)** is the difference of the value estimates between two time steps. In the temporal difference learning, only one time step ahead is done and a value estimate of the state at the next time step is usd to update the current state's value estimate."
      ]
    },
    {
      "metadata": {
        "id": "pubAX1319Okd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##TD(1) Rule\n",
        "is combined with the concept of eligibility trace.\n",
        "\n",
        "Each `Episode T` starts with:\n",
        "  * eligibility score $e(s)=0$ for all states $s$\n",
        "  * $V_T(s)=V_{T-1}(s)$, value of the state in given `Episode T` for all states $s$.\n",
        "\n",
        "Then, at each time step of the episode, update the eligibility of the state $s_{t-1}$:\n",
        "  * The state value function using temporal difference error for the current leaving\n",
        "state $s_{t-1}$ which is $r_t+\\gamma V_{T-1}(s_t)-V_{T-1}(s_{t-1})$.\n",
        "$$e(s)$$\n",
        "  * The eligibility score is discounted with the given discounting factor\n",
        "$$e(s)=\\gamma e(s)$$\n",
        "\n",
        "All these updates for all states are independent, so they can be performed in parallel. The efficiency of TD(1) method is the same as the Monte Carlo approach."
      ]
    },
    {
      "metadata": {
        "id": "R578UmPbA-0B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##TD(0) Rule\n",
        "finds the value estimate for the any finite data that is repeated infinitely. *Infinitely-repeated finite data* means to take the finite data and keep running the estimate update rule over and over again. The estimate update rule is given by:\n",
        "$$V_T(s_{t-1})=V_T(s_{t-1})+\\alpha_T[r_t+\\gamma V_{T-1}(s_t)-V_{T-1}(s_{t-1})]$$\n",
        "Therefore, the value function is given by:\n",
        "$$V_T(s_{t-1})=E_{s_{t}}[r_t+\\gamma V_{T-1}(s_t)]$$\n",
        "This is completely different from the outcome-based model, e.g. Monte Carlo, where an estimate of the state $V_T(s_t)$ is not used. TD(0) rule uses the whole sequence of rewards till the end of the episode.\n",
        "\n",
        "In the outcome-based model of the Monte Carlo approach, the value function is given by:\n",
        "$$$V_T(s_{t-1})=E[r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+......]$$\n",
        "\n",
        "The difference between TD(0) and TD(1) is the use the eligibility trace to update the state-value function in case of TD(1) but not in case of TD(0). The value function of the current leaving state $s_{t-1}$ is only updated using the temporal difference error, $r_t+\\gamma V_{T-1}(s_t)-V_{T-1}(s_{t-1})$"
      ]
    },
    {
      "metadata": {
        "id": "CvzKez2PEgDQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##TD($\\lambda$) Rule\n",
        "This is a generalized rule, where $\\lambda\\in[0,1]$ and\n",
        "* $\\lambda=0$, TD($\\lambda$) tends to TD(0)\n",
        "* $\\lambda=1$, TD($\\lambda$) tends to TD(1)\n",
        "\n",
        "The estimate update rules are:\n",
        "$$V_T(s)=V_T(s)+\\alpha_T[r_t+\\gamma V_{T-1}(s_t)-V_{T-1}(s_{t-1})]e(s)$$\n",
        "$$e(s)=\\lambda\\gamma e(s)$$"
      ]
    },
    {
      "metadata": {
        "id": "8ZUd57fPFbkF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Policy Gradients\n",
        "Recall: for any differentiable policy $\\pi_{\\theta}(s,a)$, the policy gradient is given by\n",
        "$$\\nabla_{\\theta}J(\\theta)=E_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)Q^{\\pi_{\\theta}}(s,a)]$$"
      ]
    },
    {
      "metadata": {
        "id": "if9E83shHTsl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Monte Carlo Policy Gradient\n",
        "Parameters are updated by the stochastic gradient ascent method. \\\\\n",
        "pseduocode: \\\\\n",
        "`intialize` $\\theta$ `arbitrarily` \\\\\n",
        "`for each episode as per the current policy` $\\pi_{\\theta}$ `do` \\\\\n",
        "$\\quad$ `for step t=1 to T-1 do` \\\\\n",
        "$\\quad\\quad \\theta\\leftarrow\\theta+\\alpha\\nabla_{\\theta}\\log\\pi_{\\theta}(s_t,a_t)v_t$ \\\\\n",
        "$\\quad$ `end for` \\\\\n",
        "`end for`\n",
        "`Output: final` $\\theta$\n",
        "\n",
        "where $v_t$ is an unbiased sample of $Q^{\\pi_{\\theta}}(s_t,a_t)$ and it is the cumulative reward from that time-step onward."
      ]
    },
    {
      "metadata": {
        "id": "khy3zaLKI9gf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Actor-critic algorithms\n",
        "MCPG leads to high variance. To solve this issue, a critic is used to estimate the state-action value function:\n",
        "$$Q_w(s,a)\\approx Q^{\\pi_{\\theta}}(s,a)$$\n",
        "The **actor-critic algorithm** maintains two netowrks:\n",
        "* The **critic** network updates the weight parameter $w$ of the function approximator of the state-action\n",
        "* The **actor** network updates the policy parameter $\\theta$ as per the direction given by the critic\n",
        "\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch4/actor-critic.PNG?raw=true)\n",
        "\n",
        "Therefore, the true state-action value funciton $Q^{\\pi_{\\theta}}(s,a)$ from the actual policy gradient formula is replaced with an approximate state-action value function $Q_w(s,a)$. Now the policy gradient and the update of policy parameter $\\theta$ are replaced with:\n",
        "$$\\nabla_{\\theta}J(\\theta)\\approx E_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)Q_w(s,a)]$$,\n",
        "$$\\Delta\\theta=\\alpha\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)Q_w(s,a)$$\n",
        "\n",
        "The critic network uses TD(0) rule to update the weight parameters $w$ in order to estimate the state-action value function, and the actor network uses policy gradients to update the policy parameter $\\theta$.\n",
        "\n",
        "The actor-critic algorithm uses both value-based optimzation and policy-based optimization. Unlike the Monte Carlo policy gradient approach where the policy improvement happens greedily. The actor updates the policy parameter in the actor-critic approach taking a step in the direction as per the critic in order to get a better policy."
      ]
    },
    {
      "metadata": {
        "id": "UEi2SNH793A3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Use a baseline to reduce variance\n",
        "Subtracting a **baseline function** $b(s)$ from the policy gradient to reduce variance will reduce the variance without affecting the expectation value. (Mathematical derivation is not shown)\n",
        "\n",
        "A good choice of baseline functions would be the state value function\n",
        "$$b(s)=V^{\\pi_{\\theta}}(s)$$\n",
        "Then, the policy gradient can be rewritten as:\n",
        "$$\\nabla_{\\theta}J(\\theta)=E_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)[Q^{\\pi_{\\theta}}(s,a)-V^{\\pi_{\\theta}}(s)]]$$\n",
        "where $Q^{\\pi_{\\theta}}(s,a)-V^{\\pi_{\\theta}}(s)$ is called the **advantage function** $A^{\\pi_{\\theta}}(s,a)$.\n",
        "$$\\nabla_{\\theta}J(\\theta)=E_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(s,a)A^{\\pi_{\\theta}}(s,a)]$$\n",
        "Thus, the expected value is under control by lowered variance without any change in the direction by using a baseline function."
      ]
    },
    {
      "metadata": {
        "id": "DkYHxWwCADbD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Vanilla policy gradient\n",
        "The goal of vanilla policy gradient is to update the policy using the policy gradient estimate with better baseline estimation.\n",
        "\n",
        "Pseduocode:\n",
        "\n",
        "`initialize: Policy parameter` $\\theta$ `and baseline b` \\\\\n",
        "`for iteration = 1, 2,..., N do` \\\\\n",
        "$\\quad$ `collect a set of trajectories using the current policy` \\\\\n",
        "$\\quad$ `at each time step t in each trajectory, compute the following:` \\\\\n",
        "$\\quad$ $\\quad$ `returns` $R_t=\\sum\\limits_{t'=t}^{T-1}\\gamma^{t'-t}r_{t'}$ `and` \\\\\n",
        "$\\quad$ $\\quad$ `advantage estimate` $\\hat{A}_t=R_t-b(s_t)$ \\\\\n",
        "$\\quad$ `refit the baseline function` $b(s_t)$ \\\\\n",
        "$\\quad \\delta=\\sum\\limits_{t=1}^{T-1}\\nabla_{\\theta}\\log\\pi(a_t|s_t,\\theta)\\hat{A_t}$ \\\\\n",
        "$\\quad \\theta=\\theta + \\alpha\\delta$ \\\\\n",
        "`end for`"
      ]
    },
    {
      "metadata": {
        "id": "uLgO_hIlCyr0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Agent Learning Pong Using Policy Gradients\n",
        "Environment: **pong-v0** from OpenAI gym \\\\\n",
        "Policy network: single hidden layer neural network fully connected to the raw pixels of pong at the input layer and also to the ouput layer containing a single node returning the probability of the paddle going up. \\\\\n",
        "Input image: 80x80 grayscale, not RGB\n",
        "\n",
        "Neural network:\n",
        "* **input layer**: 80x80 squashed into 6400x1, which represents 6400 nodes\n",
        "* **hidden layer**: 200 nodes\n",
        "* **output layer**: 1 node\n",
        "\n",
        "Parameters:\n",
        "* **weights and bias connecting input and hidden layer**: 6400x200 (weights) + 200 (bias)\n",
        "* **weights and bias connecting hidden and output layer**: 200x1 (weights) + 1 (bias)\n",
        "\n",
        "Note: \\\\\n",
        "The Pong environment cannot be played from a static frame, thus the motion information needs to be captured. Since the convolutional neural network is not used here, no spatial information is available. The motion information can be captured by concatenating two frames or the difference between the new and the previous frame. \\\\\n",
        "To implement reinforcement learning, try lots of tasks and note down the observations, and then perform the tasks that performed better:\n",
        "1. Initialize a random policy\n",
        "2. For the given current policy, collect different sample trajectories (rollouts) in the following way\n",
        "  1. Run a single episode of the game and capture the trajectories in that episode.\n",
        "  2. Collect a batch of trajectories.\n",
        "  \n",
        "The cases of winning a game are considered to be the correct label for the action and the cases of losing a game are wrong label. Thus, the product of advantage and log probability of actions $\\sum\\limits_{i}A_i\\times\\log p(y_i|x_i)$ are maximized after collecting the batch of trajectories.\n",
        "\n",
        " $A_i$ is the advantage associated with a state action pair. Advantage is a *scalar* quantity, which quantifies how good the action eventually turned out. $A_i$ would be high if the given action is encouraged in future and low if the action is disenrouraged. A positive advantage makes the action more likely to occur in future for that state, while a\n",
        "negative advantage makes the action less likely to occur in future for that state."
      ]
    },
    {
      "metadata": {
        "id": "aysuWujrHhMO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Implementation "
      ]
    },
    {
      "metadata": {
        "id": "I4R4_z25HmN9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import _pickle as pickle #same/load model\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PYi1RunZIndP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Hyperparameter initialization"
      ]
    },
    {
      "metadata": {
        "id": "fEFoblnOIwCB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "H = 200 #number of nodes in the hidden layer\n",
        "batch_size = 10\n",
        "learning_rate = 1e-4\n",
        "gamma = 0.99 # discount factor\n",
        "decay_rate = 0.99 # for RMPProp Optimizier -> gradient descent\n",
        "resume = False # to resume from previous checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SeH8AniEJsHI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Policy neural network model initialization"
      ]
    },
    {
      "metadata": {
        "id": "a61vYQIOJwHh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D = 80*80 # input dimension\n",
        "if resume:\n",
        "  model = pickle.load(open('model.v', 'rb'))\n",
        "else:\n",
        "  model = {}\n",
        "  # Xavier initialization of weights\n",
        "  model['W1'] = np.random.randn(H,D) * np.sqrt(2.0/D)\n",
        "  model['W2'] = np.random.randn(H) * sqrt(2.0/H)\n",
        "\n",
        "# save gradients which can be summed up over a batch\n",
        "grad_buffer = {k: np.zeros_like(v) for k,v in model.iteritems()}\n",
        "# save the value of rms prop formula\n",
        "rmsprop_cache = {k: np.zeros_like(v) for k,v in model.iteritems()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bpNLrZdpKwTl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Activation functions"
      ]
    },
    {
      "metadata": {
        "id": "UowzusVsKy2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  x[x<0] = 0\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aXOBrgwnLDnq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Preprocessing function\n",
        "takes in the image pixels as the parameter and preprocesses them by *cropping*, *downsampling*, *making* it grayscale, *erasing* the background, and *flattening* the image to a one-dimensional vector."
      ]
    },
    {
      "metadata": {
        "id": "M4YYEiEbLZHr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepro(I): # I = single frame of game as the input\n",
        "  '''preprocess 210x160x3 unit8 frame into 6400x1 (80x80) 1D float vector'''\n",
        "  I = I[35::195] # cropping only containing paddles and ball\n",
        "  I = I[::2, ::2, 0] # downsample by factor of 2 and only take R channel\n",
        "  I[I==144] = 0 # erase background type 1\n",
        "  I[I==109] = 0 # erase background type 2\n",
        "  I[I!=0] = 1 # everything else set of 1 except paddles and ball\n",
        "  return I.astype('float').ravel() # flatten to 1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z91M-wX0MZQK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Discount rewards\n",
        "takes a list of rewards $r$ corresponding to different time-steps as the parameters and returns a list of discounted rewards corresponding to different time-steps"
      ]
    },
    {
      "metadata": {
        "id": "vIxUOykDMs1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discount_rewards(r):\n",
        "  '''take 1D float array of rewards and compute discounted reward'''\n",
        "  discount_r = np.zeros_like(r)\n",
        "  running_add = 0 # addition of rewards\n",
        "  for t in reversed(xrange(0, r.size)):\n",
        "    if r[t] != 0: # episode ends\n",
        "      running_add = 0\n",
        "    running_add = gamma*running_add + r[t]\n",
        "    discount_r[t] = running_add\n",
        "  return discount_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QsgCG6nhOVov",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Forward propagation\n",
        "takes in the preprocessed image vector and returns the probability of action being `UP` and a vector containing the value of hidden state nodes"
      ]
    },
    {
      "metadata": {
        "id": "VUXkA0CyOmxf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h = relu(h)\n",
        "  logit = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logit)\n",
        "  return p, h\n",
        "  #probability of action 2(i.e. UP) and hidden layer state (i.e. hidden state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_qoRRKO_O7as",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Backward propagation\n",
        "takesin the hidden state values, error `gradient_logp`, and observations to compute the derivatives with respect to different weight parameters."
      ]
    },
    {
      "metadata": {
        "id": "Cx5K1JNwPM3_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def policy_backward(arr_hidden_state, gradient_logp, observation_values):\n",
        "  '''backward pass'''\n",
        "  # arr_hidden_state is array of intermediate hidden states   shape[200x1]\n",
        "  # gradient_logp is the loss value   shape[1x1]\n",
        "  \n",
        "  dW2 = np.dot(arr_hidden_state.T, gradient_logp).ravel()\n",
        "  # [200x1].[1x1] => [200x1] => flatten => [1x200]\n",
        "  \n",
        "  dh = np.outer(gradient_logp, model['W2'])\n",
        "  # [1x1]outer[1x200] => [1x200]\n",
        "  \n",
        "  dh = relu(dh)\n",
        "  \n",
        "  dW1 = np.dot(dh.T, observation_values)\n",
        "  # [200x1].[1x6400] => [200x6400]\n",
        "  \n",
        "  return {'W1':dW1, 'W2':dW2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fDA8se03Q0OX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Main implementation and Training"
      ]
    },
    {
      "metadata": {
        "id": "VDwcb2wiQ3aY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('Pong-v0')\n",
        "observation = env.reset()\n",
        "prev_x = None\n",
        "'''\n",
        "prev frame value in order to compute the difference between current and previous frame\n",
        "#as discussed frames are static and the difference is used to capture the motion\n",
        "#Intially None because there's no previous frame if the current frame is the 1st frame of the game\n",
        "'''\n",
        "episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [],[],[],[]\n",
        "running_reward = None\n",
        "reward_sum = 0\n",
        "episode_number = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3iooaJzLRhtd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training\n",
        "while True:\n",
        "  env.render()\n",
        "  # get the input and preprocess it\n",
        "  cur_x = prepro(observation)\n",
        "  # get the frame difference which would be the input to the network\n",
        "  if prev_x is None:\n",
        "    prev_x = np.zeros(D)\n",
        "  x = cur_x - prev_x\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  #forward propagation of the policy network\n",
        "  #sample an action from the returned probability\n",
        "  aprob, h = policy_forward(x)\n",
        "  #stochastic process\n",
        "  if np.random.uniform() < aprob:\n",
        "    action = 2\n",
        "  else:\n",
        "    action = 3\n",
        "    \n",
        "  episode_observation.append(x) # record observation\n",
        "  episode_hidden_layer_values.append(h) # record hidden state\n",
        "  if action == 2:\n",
        "    y = 1\n",
        "  else:\n",
        "    y = 0\n",
        "    \n",
        "  episode_gradient_log_ps.append(y-aprob) # record the gradient\n",
        "  \n",
        "  #new step in the environment\n",
        "  observation, reward, done, info = env.step(action)\n",
        "  reward_sum+=reward #for advantage purpose\n",
        "  episode_rewards.append(reward) # record the reward\n",
        "  \n",
        "  if done: # if the episode is over\n",
        "    episode_number += 1\n",
        "    \n",
        "    # stack inputs, hidden_states, actions, gradients_logp, rewards for the episode\n",
        "    arr_hidden_state = np.vstack(episode_hidden_layer_values)\n",
        "    gradient_logp = np.vstack(episode_gradient_log_ps)\n",
        "    observation_values = np.vstack(episode_observations)\n",
        "    reward_values = np.vstack(episode_rewards)\n",
        "    \n",
        "    # reset the memory arrays\n",
        "    episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [],[],[],[]\n",
        "    \n",
        "    # discounted reward computation\n",
        "    discounted_episoderewards = discount_rewards(reward_values)\n",
        "    # normalize discounted_episoderewards i.e. obtain advantage\n",
        "    discounted_episoderewards = (discounted_episoderewards - np.mean(discounted_episoderewards)) / np.std(discounted_episoderewards)\n",
        "    \n",
        "    # modulate the gradient with the advantage\n",
        "    gradient_logp *= discounted_episoderewards\n",
        "    \n",
        "    grad = policy_backward(arr_hidden_state, gradient_logp, observation_values)\n",
        "    \n",
        "    # sum the gradient over the batch size\n",
        "    for layer in model:\n",
        "      grad_buffer[layer] += grad[layer]\n",
        "      \n",
        "    # perform RMSProp to update weights after every 10 episodes\n",
        "    if episode_number % batch_size == 0:\n",
        "      epsilon = 1e-5\n",
        "      for weight in model.keys():\n",
        "        g = grad_buffer[weight] # gradient\n",
        "        rmsprop_cache[weight] = decay_rate*rmsprop_cache[weight] + (1-decay_rate)*g**2\n",
        "        model[weight] += learning_rate*g / (np.sqrt(rmsprop_cache[weight]) + epsilon)\n",
        "        grad_buffer[weight] = np.zeros_like(model[weight])\n",
        "        \n",
        "    if running_reward is None:\n",
        "      running_reward = reward_sum\n",
        "    else:\n",
        "      running_reward = running_reward*learning_rate+reward_sum*(1-learning_rate)\n",
        "\n",
        "    print('Episode Reward : {}, Running Mean Award : {}'.format(reward_sum,running_reward))\n",
        "    if episode_number % 100 == 0:\n",
        "      pickle.dump(model,open('model.v','wb'))\n",
        "\n",
        "    reward_sum = 0\n",
        "    prev_x = None\n",
        "    observation = env.reset() # reset the environment since episode has ended\n",
        "    \n",
        "  if reward != 0: # if reward is either +1 or -1 i.e. an episode has ended\n",
        "    print('Episode {} ended with reward {}'.format(episode_number, reward))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pwt8r9L5xK0x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To accelerate the training process, enable the GPU computing setting. It may still take 5-7 hours to complete the training."
      ]
    }
  ]
}