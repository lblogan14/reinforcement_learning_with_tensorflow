{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch6_asynchronous.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/reinforcement_learning_with_tensorflow/blob/master/ch6_asynchronous.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wce1m2zh2OAo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Review:\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch4/actor-critic.PNG?raw=true)\n",
        "\n",
        "Recall the structure of the actor-critic algorithm from Chapter 4. \\\\\n",
        "The **Actor** takes the current environment state and determines best action to take; \\\\\n",
        "the **Critic** plays a policy-evaluation role by taking in the environment state and action, and then returns a score depicting how good an action is for the state.\n",
        "\n",
        "Thus, the actor-critic algorithm learns both the policy and state-action value function."
      ]
    },
    {
      "metadata": {
        "id": "scb_Z4Wq3S9C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asynchronous Methods\n",
        "Deep Q-network utilizes the experience replay to train the deep neural network in order to find out the maximum Q-value for the most favorable action, but it takes too much memory usage and heavy computation over time. Thus, the asynchronous method is to overcome this issue. Instead of using experience replay, in asynchronous methods, multiple instances of the environment are created and multiple agents asynchronously execute actions in parallel:\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch6/asy_method.PNG?raw=true)\n",
        "\n",
        "Thus, each thread is assigned the process that contains a learner representing an agent network that interacts with its own copy of the environment. Multiple learners run in parallel exploring their own environment. The parallelism allows the agent to experience varied different states simultaneously at any given time-step, and covers the fundamentals of both off-policy and on-policy learning algorithms. These multiple learners running in parallel use different exploration policies, which maximizes the diversity. Different exploration policies by different learners changes the parameters, and these updates have the least chance to be correlated in time. Therefore, experience replay memory is not required.\n",
        "\n",
        "Examples of asynchronous methods:\n",
        "* Asynchronous one-step Q-learning\n",
        "* Asynchronous one-step SARSA\n",
        "* Asynchronous n-step Q-learning\n",
        "* Asynchronous advantage actor critic (A3C)"
      ]
    },
    {
      "metadata": {
        "id": "xCu1MGPL6C8c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asynchronous one-step Q-learning\n",
        "An agent in DQN is represented by a set of primary and target networks, where **one-step loss** is calculated as the square of the difference between the state-action value of the current state s predicted by the primary network and the target state-action value of the current state calculated by the target network. (Similar to DQN)\n",
        "\n",
        "(New for Asynchronous) There are multiple learning agents running and calculating the one-step loss in parallel. Thus the gradient calculation occurs in parallel in different threads where each learning agent interacts with its own copy of environment. The accumulation of these gradients in\n",
        "different threads over multiple time steps are used to update the policy network parameters\n",
        "after a fixed time step, or when an episode is over. The accumulation of gradients is\n",
        "preferred over policy network parameter updates because this avoids overwriting the\n",
        "changes perform by each of the learner agents.\n",
        "\n",
        "Adding a different exploration policy to different threads makes the learning\n",
        "diverse and robust. This improves the performance owing to better exploration, because\n",
        "each of the learning agents in different threads is subjected to a different exploration policy.\n",
        "\n",
        "Pseudocode for Asynchronous one-step Q-learning: \\\\\n",
        "where \\\\\n",
        "* $\\theta$: parameters of the policy network\n",
        "* $\\theta^t$: parameters of the target network\n",
        "* $T$: overal time step counter\n",
        "\n",
        "`// Globally shared parameters ` $\\theta, \\theta^t$ and $T$ \\\\\n",
        "`// ` $\\theta$ `is initialized arbitrarily` \\\\\n",
        "`// T is initialized 0` \\\\\n",
        " \\\\\n",
        "`Initialize thread level time step counter t=0` \\\\\n",
        "`Initialize ` $\\theta^t=\\theta$ \\\\\n",
        "`Initialize network gradients` $d\\theta=0$ \\\\\n",
        "`Start with the initial state s` \\\\\n",
        "`repeat until ` $T>T_{\\max}:$ \\\\\n",
        "$\\quad$ `Choose action a with ` $\\epsilon$ `-greedy policy such that:` \\\\\n",
        "$\\quad\\quad a=\\{\\begin{array}{l}a\\,random\\, action\\quad ,with\\,probability\\,\\epsilon \\\\ \\arg\\max_{a'}Q(\\phi(s),a';\\theta) \\quad otherwise \\end{array}$ \\\\\n",
        "$\\quad$ `Perform action a` \\\\\n",
        "$\\quad$ `Receive new state s' and reward r` \\\\\n",
        "$\\quad$ `Compute target y: ` $y = \\{ \\begin{array}{l}\n",
        "                                                 r \\quad , for\\,terminal\\,s'\\\\ \n",
        "                                                 r+\\gamma\\max_{a'}Q(s',a';\\theta_t) \\quad ,otherwise \n",
        "                                                 \\end{array}$ \\\\\n",
        "$\\quad$ `Compute the loss, ` $L(\\theta)=(y-Q(s,a;\\theta))^2$ \\\\\n",
        "$\\quad$ `Accumulate the gradient w.r.t. ` $\\theta : d\\theta=d\\theta+\\frac{\\nabla L(\\theta)}{\\nabla\\theta}$ \\\\\n",
        "$\\quad$ `s = s'` \\\\\n",
        "$\\quad$ `T = T + 1` \\\\\n",
        "$\\quad$ `t = t + 1` \\\\\n",
        " \\\\\n",
        "$\\quad$ `if T mod ` $I_{target}==0:$ \\\\\n",
        "$\\quad\\quad$ `Update the parameters of target network: ` $\\theta^t=\\theta$ \\\\\n",
        "$\\quad\\quad$ `# After every ` $I_{target}$ `time steps the parameters of target network is updated` \\\\\n",
        " \\\\\n",
        "$\\quad$ `if t mod ` $I_{AsyncUpdate}==0$ `or s = terminal state:` \\\\\n",
        "$\\quad\\quad$ `Asynchronous update of ` $\\theta$ `using ` $d\\theta$ \\\\\n",
        "$\\quad\\quad$ `Clear gradients: ` $d\\theta=0$ \\\\\n",
        "$\\quad\\quad$ `#at every ` $I_{AsyncUpdate}$ `time step in the thread or if s is the terminal state` \\\\\n",
        "$\\quad\\quad$ `# update ` $\\theta$ `using accumulated gradients ` $d\\theta$"
      ]
    },
    {
      "metadata": {
        "id": "nxaetYYAAMK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asynchronous one-step SARSA\n",
        "uses $\\epsilon$-greedy to choose the action $a'$ for the next state $s'$ and the Q-value of the next state-action pair: $Q(s',a'; \\theta^t)$ is used to calculate the target state-action value of the current state.\n",
        "\n",
        "Pseudocode for Asynchronous one-step SARASA: \\\\\n",
        "where \\\\\n",
        "* $\\theta$: parameters of the policy network\n",
        "* $\\theta^t$: parameters of the target network\n",
        "* $T$: overal; time step counter\n",
        "\n",
        "`// Globally shared parameters ` $\\theta, \\theta^t$ and $T$ \\\\\n",
        "`// ` $\\theta$ `is initialized arbitrarily` \\\\\n",
        "`// T is initialized 0` \\\\\n",
        " \\\\\n",
        "`Initialize thread level time step counter t=0` \\\\\n",
        "`Initialize ` $\\theta^t=\\theta$ \\\\\n",
        "`Initialize network gradients` $d\\theta=0$ \\\\\n",
        "`Start with the initial state s` \\\\\n",
        "`Choose action a with ` $\\epsilon$ `-greedy policy such that:` \\\\\n",
        "$\\quad a=\\{\\begin{array}{l}a\\,random\\, action\\quad ,with\\,probability\\,\\epsilon \\\\ \\arg\\max_{a'}Q(\\phi(s),a';\\theta) \\quad otherwise \\end{array}$ \\\\\n",
        "`repeat until ` $T>T_{\\max}$ : \\\\\n",
        "$\\quad$ `Perform action a` \\\\\n",
        "$\\quad$ `Receive new state s' and reward r` \\\\\n",
        "$\\quad$ `Choose action a' with ` $\\epsilon$ `-greedy policy such that:` \\\\\n",
        "$\\quad\\quad a'=\\{\\begin{array}{l}a\\,random\\, action\\quad ,with\\,probability\\,\\epsilon \\\\ \\arg\\max_{a'}Q(\\phi(s),a'';\\theta) \\quad otherwise \\end{array}$ \\\\\n",
        "$\\quad$ `Compute target y: ` $y = \\{ \\begin{array}{l}\n",
        "                                                 r \\quad , for\\,terminal\\,s'\\\\ \n",
        "                                                 r+\\gamma\\max_{a'}Q(s',a';\\theta_t) \\quad ,otherwise \n",
        "                                                 \\end{array}$ \\\\\n",
        "$\\quad$ `Compute the loss, ` $L(\\theta)=(y-Q(s,a;\\theta))^2$ \\\\\n",
        "$\\quad$ `Accumulate the gradient w.r.t. ` $\\theta : d\\theta=d\\theta+\\frac{\\nabla L(\\theta)}{\\nabla\\theta}$ \\\\\n",
        "$\\quad$ `s = s'` \\\\\n",
        "$\\quad$ `T = T + 1` \\\\\n",
        "$\\quad$ `t = t + 1` \\\\\n",
        "$\\quad$ `a = a'` \\\\\n",
        " \\\\\n",
        "$\\quad$ `if T mod ` $I_{target}==0:$ \\\\\n",
        "$\\quad\\quad$ `Update the parameters of target network: ` $\\theta^t=\\theta$ \\\\\n",
        "$\\quad\\quad$ `# After every ` $I_{target}$ `time steps the parameters of target network is updated` \\\\\n",
        " \\\\\n",
        "$\\quad$ `if t mod ` $I_{AsyncUpdate}==0$ `or s = terminal state:` \\\\\n",
        "$\\quad\\quad$ `Asynchronous update of ` $\\theta$ `using ` $d\\theta$ \\\\\n",
        "$\\quad\\quad$ `Clear gradients: ` $d\\theta=0$ \\\\\n",
        "$\\quad\\quad$ `#at every ` $I_{AsyncUpdate}$ `time step in the thread or if s is the terminal state` \\\\\n",
        "$\\quad\\quad$ `# update ` $\\theta$ `using accumulated gradients ` $d\\theta$"
      ]
    },
    {
      "metadata": {
        "id": "H4YmuHvbFrxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asynchronous n-step Q-learning\n",
        "similar to asynchronous one-step Q-learning, but for asynchronous n-step Q-learning, the learning agent actions are selected using the exploration policy for up to $t_{\\max}$ steps or until a terminal state is reached, in order to compute a single update of policy network parameters. The loss for each time step is calculated as the difference between the discounted future rewards at that time step and the estimated Q-value.\n",
        "\n",
        "The loss gradient with respect to thread-specific network parameters for each time step is calculated and accumulated. There are multiple such\n",
        "learning agents running and accumulating the gradients in parallel. These accumulated\n",
        "gradients are used to perform asynchronous updates of policy network parameters.\n",
        "\n",
        "Pseudocode for asynchronous n-step Q-learning: \\\\\n",
        "where \\\\\n",
        "* $\\theta$: parameters of the policy network\n",
        "* $\\theta^t$: parameters of the target network\n",
        "* $T$: overall time step counter\n",
        "* $t$: thread level time step counter\n",
        "* $T_{\\max}$: maximum number of overall time steps\n",
        "* $t_{\\max}$: maximum number of time steps in a thread\n",
        "\n",
        "`// Globally shared parameters ` $\\theta, \\theta^t$ and $T$ \\\\\n",
        "`// ` $\\theta$ `is initialized arbitrarily` \\\\\n",
        "`// T is initialized 0` \\\\\n",
        " \\\\\n",
        "`Initialize thread level time step counter t=0` \\\\\n",
        "`Initialize ` $\\theta^t=\\theta$ \\\\\n",
        "`Initialize ` $\\theta'=\\theta$ \\\\\n",
        "`Initialize network gradients` $d\\theta=0$ \\\\\n",
        " \\\\\n",
        "`repeat until ` $T>T_{\\max}$: \\\\\n",
        "$\\quad$ `Clear gradient: ` $d\\theta=0$ \\\\\n",
        "$\\quad$ `Synchronize thread-specific parameters: ` $\\theta'=\\theta$ \\\\\n",
        "$\\quad t_{start}=t$ \\\\\n",
        "$\\quad$ `Get state ` $s_t$ \\\\\n",
        "$\\quad$ `r = [] // list of rewards` \\\\\n",
        "$\\quad$  `a = [] // list of actions` \\\\\n",
        "$\\quad$ `s = [] // list of states` \\\\\n",
        "$\\quad$ `repeat until ` $s_t$ `is a terminal state or ` $t-t_{start}==t_{\\max}$: \\\\\n",
        "$\\quad\\quad$ `Choose action` $a_t$  `with ` $\\epsilon$ `-greedy policy such that:` \\\\\n",
        "$\\quad\\quad\\quad a_t=\\{\\begin{array}{l}a\\,random\\, action\\quad ,with\\,probability\\,\\epsilon \\\\ \\arg\\max_{a_t}Q(\\phi(s),a'';\\theta) \\quad otherwise \\end{array}$ \\\\\n",
        "$\\quad\\quad$ `Perform action` $a_t$ \\\\\n",
        "$\\quad\\quad$ `Receive new state ` $s_{t+1}$ `and reward ` $r_t$ \\\\\n",
        "$\\quad\\quad$ `Accumulate rewards by appending ` $r_t$ `to r` \\\\\n",
        "$\\quad\\quad$ `Accumulate actions by appending ` $a_t$ `to a` \\\\\n",
        "$\\quad\\quad$ `Accumulate states by appending ` $s_t$ `to s` \\\\\n",
        "$\\quad\\quad$ `t = t + 1` \\\\\n",
        "$\\quad\\quad$ `T = T + 1` \\\\\n",
        "$\\quad\\quad s_t=s_{t+1}$ \\\\\n",
        "$\\quad$ `Compute returns, R:` $R=\\{\\begin{array}{l}0\\quad ,\\,for\\,terminal\\,s_t \\\\ \\max_{a}Q(s_t,a;\\theta^t) \\quad otherwise \\end{array}$ \\\\\n",
        "$\\quad$ `for ` $i\\in[t-1,......, t_{start}]$ `do:` \\\\\n",
        "$\\quad\\quad R=r_i+\\gamma R$ \\\\\n",
        "$\\quad\\quad$ `Compute loss, ` $L(\\theta')=(R-Q(s_i,a_i;\\theta'))^2$ \\\\\n",
        "$\\quad\\quad$ `Accumulate gradients w.r.t. ` $\\theta': d\\theta=d\\theta+\\frac{\\nabla L(\\theta')}{\\nabla\\theta'}$ \\\\\n",
        "$\\quad$ `Asynchronous update of ` $\\theta$ `using ` $d\\theta$ \\\\\n",
        "$\\quad$ `if T mod ` $I_{target}==0:$ \\\\\n",
        "$\\quad\\quad$ `Update the parameters of target network: ` $\\theta^t=\\theta$ \\\\\n",
        "$\\quad\\quad$ `# After every ` $I_{target}$ `time steps the parameters of target network is updated` \\\\"
      ]
    }
  ]
}