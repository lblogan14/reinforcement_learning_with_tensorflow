{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch6_asynchronous.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/reinforcement_learning_with_tensorflow/blob/master/ch6_asynchronous.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wce1m2zh2OAo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Review:\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch4/actor-critic.PNG?raw=true)\n",
        "\n",
        "Recall the structure of the actor-critic algorithm from Chapter 4. \\\\\n",
        "The **Actor** takes the current environment state and determines best action to take; \\\\\n",
        "the **Critic** plays a policy-evaluation role by taking in the environment state and action, and then returns a score depicting how good an action is for the state.\n",
        "\n",
        "Thus, the actor-critic algorithm learns both the policy and state-action value function."
      ]
    },
    {
      "metadata": {
        "id": "scb_Z4Wq3S9C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asynchronous Methods\n",
        "Deep Q-network utilizes the experience replay to train the deep neural network in order to find out the maximum Q-value for the most favorable action, but it takes too much memory usage and heavy computation over time. Thus, the asynchronous method is to overcome this issue. Instead of using experience replay, in asynchronous methods, multiple instances of the environment are created and multiple agents asynchronously execute actions in parallel:\n",
        "![alt text](https://github.com/lblogan14/reinforcement_learning_with_tensorflow/blob/master/note_images/ch6/asy_method.PNG?raw=true)\n",
        "\n",
        "Thus, each thread is assigned the process that contains a learner representing an agent network that interacts with its own copy of the environment. Multiple learners run in parallel exploring their own environment. The parallelism allows the agent to experience varied different states simultaneously at any given time-step, and covers the fundamentals of both off-policy and on-policy learning algorithms. These multiple learners running in parallel use different exploration policies, which maximizes the diversity. Different exploration policies by different learners changes the parameters, and these updates have the least chance to be correlated in time. Therefore, experience replay memory is not required.\n",
        "\n",
        "Examples of asynchronous methods:\n",
        "* Asynchronous one-step Q-learning\n",
        "* Asynchronous one-step SARSA\n",
        "* Asynchronous n-step Q-learning\n",
        "* Asynchronous advantage actor critic (A3C)"
      ]
    },
    {
      "metadata": {
        "id": "xCu1MGPL6C8c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asynchronous one-step Q-learning\n",
        "An agent in DQN is represented by a set of primary and target networks, where **one-step loss** is calculated as the square of the difference between the state-action value of the current state s predicted by the primary network and the target state-action value of the current state calculated by the target network. (Similar to DQN)\n",
        "\n",
        "(New for Asynchronous) There are multiple learning agents running and calculating the one-step loss in parallel. Thus the gradient calculation occurs in parallel in different threads where each learning agent interacts with its own copy of environment. The accumulation of these gradients in\n",
        "different threads over multiple time steps are used to update the policy network parameters\n",
        "after a fixed time step, or when an episode is over. The accumulation of gradients is\n",
        "preferred over policy network parameter updates because this avoids overwriting the\n",
        "changes perform by each of the learner agents.\n",
        "\n",
        "Adding a different exploration policy to different threads makes the learning\n",
        "diverse and robust. This improves the performance owing to better exploration, because\n",
        "each of the learning agents in different threads is subjected to a different exploration policy.\n",
        "\n",
        "Pseudocode for Asynchronous one-step Q-learning: \\\\\n",
        "where \\\\\n",
        "* $\\theta$: parameters of the policy network\n",
        "* $\\theta^t$: parameters of the target network\n",
        "* $T$: overal time step counter\n",
        "\n",
        "`// Globally shared parameters ` $\\theta, \\theta^t$ and $T$ \\\\\n",
        "`// ` $\\theta$ `is initialized arbitrarily` \\\\\n",
        "`// T is initialized 0` \\\\\n",
        " \\\\\n",
        "`Initialize thread level time step counter t=0` \\\\\n",
        "`Initialize ` $\\theta^t=\\theta$ \\\\\n",
        "`Initialize network gradients` $d\\theta=0$ \\\\\n",
        "`Start with the initial state s` \\\\\n",
        "`repeat until ` $T>T_{\\max}:$ \\\\\n",
        "$\\quad$ `Choose action a with ` $\\epsilon$ `-greedy policy such that:` \\\\\n",
        "$\\quad\\quad a=\\{\\begin{array}{l}a\\,random\\, action\\quad ,with\\,probability\\,\\epsilon \\\\ \\arg\\max_{a'}Q(\\phi(s),a';\\theta) \\quad otherwise \\end{array}$ \\\\\n",
        "$\\quad$ `Perform action a` \\\\\n",
        "$\\quad$ `Receive new state s' and reward r` \\\\\n",
        "$\\quad$ `Compute target y: ` $y = \\{ \\begin{array}{l}\n",
        "                                                 r \\quad , for\\,terminal\\,s'\\\\ \n",
        "                                                 r+\\gamma\\max_{a'}Q(s',a';\\theta_t) \\quad ,otherwise \n",
        "                                                 \\end{array}$ \\\\\n",
        "$\\quad$ `Compute the loss, ` $L(\\theta)=(y-Q(s,a;\\theta))^2$ \\\\\n",
        "$\\quad$ `Accumulate the gradient w.r.t. ` $\\theta : d\\theta=d\\theta+\\frac{\\nabla L(\\theta)}{\\nabla\\theta}$ \\\\\n",
        "$\\quad$ `s = s'` \\\\\n",
        "$\\quad$ `T = T + 1` \\\\\n",
        "$\\quad$ `t = t + 1` \\\\\n",
        " \\\\\n",
        "$\\quad$ `if T mod ` $I_{target}==0:$ \\\\\n",
        "$\\quad\\quad$ `Update the parameters of target network: ` $\\theta^t=\\theta$ \\\\\n",
        "$\\quad\\quad$ `# After every ` $I_{target}$ `time steps the parameters of target network is updated` \\\\\n",
        " \\\\\n",
        "$\\quad$ `if t mod ` $I_{AsyncUpdate}==0$ `or s = terminal state:` \\\\\n",
        "$\\quad\\quad$ `Asynchronous update of ` $\\theta$ `using ` $d\\theta$ \\\\\n",
        "$\\quad\\quad$ `Clear gradients: ` $d\\theta=0$ \\\\\n",
        "$\\quad\\quad$ `#at every ` $I_{AsyncUpdate}$ `time step in the thread or if s is the terminal state` \\\\\n",
        "$\\quad\\quad$ `# update ` $\\theta$ `using accumulated gradients ` $d\\theta$"
      ]
    },
    {
      "metadata": {
        "id": "nxaetYYAAMK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asynchronous one-step SARSA\n",
        "uses $\\epsilon$-greedy to choose the action $a'$ for the next state $s'$ and the Q-value of the next state-action pair: $Q(s',a'; \\theta^t)$ is used to calculate the target state-action value of the current state.\n",
        "\n",
        "Pseudocode for Asynchronous one-step SARASA: \\\\\n",
        "where \\\\\n",
        "* $\\theta$: parameters of the policy network\n",
        "* $\\theta^t$: parameters of the target network\n",
        "* $T$: overal; time step counter\n",
        "\n",
        "`// Globally shared parameters ` $\\theta, \\theta^t$ and $T$ \\\\\n",
        "`// ` $\\theta$ `is initialized arbitrarily` \\\\\n",
        "`// T is initialized 0` \\\\\n",
        " \\\\\n",
        "`Initialize thread level time step counter t=0` \\\\\n",
        "`Initialize ` $\\theta^t=\\theta$ \\\\\n",
        "`Initialize network gradients` $d\\theta=0$ \\\\\n",
        "`Start with the initial state s` \\\\\n",
        "`Choose action a with ` $\\epsilon$ `-greedy policy such that:` \\\\\n",
        "$\\quad a=\\{\\begin{array}{l}a\\,random\\, action\\quad ,with\\,probability\\,\\epsilon \\\\ \\arg\\max_{a'}Q(\\phi(s),a';\\theta) \\quad otherwise \\end{array}$ \\\\\n",
        "`repeat until ` $T>T_{\\max}$ : \\\\\n",
        "$\\quad$ `Perform action a` \\\\\n",
        "$\\quad$ `Receive new state s' and reward r` \\\\\n",
        "$\\quad$ `Choose action a' with ` $\\epsilon$ `-greedy policy such that:` \\\\\n",
        "$\\quad\\quad a'=\\{\\begin{array}{l}a\\,random\\, action\\quad ,with\\,probability\\,\\epsilon \\\\ \\arg\\max_{a'}Q(\\phi(s),a'';\\theta) \\quad otherwise \\end{array}$ \\\\\n",
        "$\\quad$ `Compute target y: ` $y = \\{ \\begin{array}{l}\n",
        "                                                 r \\quad , for\\,terminal\\,s'\\\\ \n",
        "                                                 r+\\gamma\\max_{a'}Q(s',a';\\theta_t) \\quad ,otherwise \n",
        "                                                 \\end{array}$ \\\\\n",
        "$\\quad$ `Compute the loss, ` $L(\\theta)=(y-Q(s,a;\\theta))^2$ \\\\\n",
        "$\\quad$ `Accumulate the gradient w.r.t. ` $\\theta : d\\theta=d\\theta+\\frac{\\nabla L(\\theta)}{\\nabla\\theta}$ \\\\\n",
        "$\\quad$ `s = s'` \\\\\n",
        "$\\quad$ `T = T + 1` \\\\\n",
        "$\\quad$ `t = t + 1` \\\\\n",
        "$\\quad$ `a = a'` \\\\\n",
        " \\\\\n",
        "$\\quad$ `if T mod ` $I_{target}==0:$ \\\\\n",
        "$\\quad\\quad$ `Update the parameters of target network: ` $\\theta^t=\\theta$ \\\\\n",
        "$\\quad\\quad$ `# After every ` $I_{target}$ `time steps the parameters of target network is updated` \\\\\n",
        " \\\\\n",
        "$\\quad$ `if t mod ` $I_{AsyncUpdate}==0$ `or s = terminal state:` \\\\\n",
        "$\\quad\\quad$ `Asynchronous update of ` $\\theta$ `using ` $d\\theta$ \\\\\n",
        "$\\quad\\quad$ `Clear gradients: ` $d\\theta=0$ \\\\\n",
        "$\\quad\\quad$ `#at every ` $I_{AsyncUpdate}$ `time step in the thread or if s is the terminal state` \\\\\n",
        "$\\quad\\quad$ `# update ` $\\theta$ `using accumulated gradients ` $d\\theta$"
      ]
    },
    {
      "metadata": {
        "id": "H4YmuHvbFrxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asynchronous n-step Q-learning\n",
        "similar to asynchronous one-step Q-learning, but for asynchronous n-step Q-learning, the learning agent actions are selected using the exploration policy for up to $t_{\\max}$ steps or until a terminal state is reached, in order to compute a single update of policy network parameters. The loss for each time step is calculated as the difference between the discounted future rewards at that time step and the estimated Q-value.\n",
        "\n",
        "The loss gradient with respect to thread-specific network parameters for each time step is calculated and accumulated. There are multiple such\n",
        "learning agents running and accumulating the gradients in parallel. These accumulated\n",
        "gradients are used to perform asynchronous updates of policy network parameters.\n",
        "\n",
        "Pseudocode for asynchronous n-step Q-learning: \\\\\n",
        "where \\\\\n",
        "* $\\theta$: parameters of the policy network\n",
        "* $\\theta^t$: parameters of the target network\n",
        "* $T$: overall time step counter\n",
        "* $t$: thread level time step counter\n",
        "* $T_{\\max}$: maximum number of overall time steps\n",
        "* $t_{\\max}$: maximum number of time steps in a thread\n",
        "\n",
        "`// Globally shared parameters ` $\\theta, \\theta^t$ and $T$ \\\\\n",
        "`// ` $\\theta$ `is initialized arbitrarily` \\\\\n",
        "`// T is initialized 0` \\\\\n",
        " \\\\\n",
        "`Initialize thread level time step counter t=0` \\\\\n",
        "`Initialize ` $\\theta^t=\\theta$ \\\\\n",
        "`Initialize ` $\\theta'=\\theta$ \\\\\n",
        "`Initialize network gradients` $d\\theta=0$ \\\\\n",
        " \\\\\n",
        "`repeat until ` $T>T_{\\max}$: \\\\\n",
        "$\\quad$ `Clear gradient: ` $d\\theta=0$ \\\\\n",
        "$\\quad$ `Synchronize thread-specific parameters: ` $\\theta'=\\theta$ \\\\\n",
        "$\\quad t_{start}=t$ \\\\\n",
        "$\\quad$ `Get state ` $s_t$ \\\\\n",
        "$\\quad$ `r = [] // list of rewards` \\\\\n",
        "$\\quad$  `a = [] // list of actions` \\\\\n",
        "$\\quad$ `s = [] // list of states` \\\\\n",
        "$\\quad$ `repeat until ` $s_t$ `is a terminal state or ` $t-t_{start}==t_{\\max}$: \\\\\n",
        "$\\quad\\quad$ `Choose action` $a_t$  `with ` $\\epsilon$ `-greedy policy such that:` \\\\\n",
        "$\\quad\\quad\\quad a_t=\\{\\begin{array}{l}a\\,random\\, action\\quad ,with\\,probability\\,\\epsilon \\\\ \\arg\\max_{a_t}Q(\\phi(s),a'';\\theta) \\quad otherwise \\end{array}$ \\\\\n",
        "$\\quad\\quad$ `Perform action` $a_t$ \\\\\n",
        "$\\quad\\quad$ `Receive new state ` $s_{t+1}$ `and reward ` $r_t$ \\\\\n",
        "$\\quad\\quad$ `Accumulate rewards by appending ` $r_t$ `to r` \\\\\n",
        "$\\quad\\quad$ `Accumulate actions by appending ` $a_t$ `to a` \\\\\n",
        "$\\quad\\quad$ `Accumulate states by appending ` $s_t$ `to s` \\\\\n",
        "$\\quad\\quad$ `t = t + 1` \\\\\n",
        "$\\quad\\quad$ `T = T + 1` \\\\\n",
        "$\\quad\\quad s_t=s_{t+1}$ \\\\\n",
        "$\\quad$ `Compute returns, R:` $R=\\{\\begin{array}{l}0\\quad ,\\,for\\,terminal\\,s_t \\\\ \\max_{a}Q(s_t,a;\\theta^t) \\quad otherwise \\end{array}$ \\\\\n",
        "$\\quad$ `for ` $i\\in[t-1,......, t_{start}]$ `do:` \\\\\n",
        "$\\quad\\quad R=r_i+\\gamma R$ \\\\\n",
        "$\\quad\\quad$ `Compute loss, ` $L(\\theta')=(R-Q(s_i,a_i;\\theta'))^2$ \\\\\n",
        "$\\quad\\quad$ `Accumulate gradients w.r.t. ` $\\theta': d\\theta=d\\theta+\\frac{\\nabla L(\\theta')}{\\nabla\\theta'}$ \\\\\n",
        "$\\quad$ `Asynchronous update of ` $\\theta$ `using ` $d\\theta$ \\\\\n",
        "$\\quad$ `if T mod ` $I_{target}==0:$ \\\\\n",
        "$\\quad\\quad$ `Update the parameters of target network: ` $\\theta^t=\\theta$ \\\\\n",
        "$\\quad\\quad$ `# After every ` $I_{target}$ `time steps the parameters of target network is updated` \\\\"
      ]
    },
    {
      "metadata": {
        "id": "AAZczCKUp4l6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asynchronous Advantage Actor Critic (A3C)\n",
        "Each learning agent contains an actor-critic learner that combines the benefits of both value-based and policy-based methods. \\\\\n",
        "The\n",
        "actor network takes in the state as input and predicts the best action of that state, while the\n",
        "critic network takes in the state and action as the inputs and outputs the action score to\n",
        "quantify how good the action is for that state. The actor network updates its weight\n",
        "parameters using policy gradients, while the critic network updates its weight parameters using *TD(0)* method, the difference of value estimates between two time steps. \\\\\n",
        "Recall that the **advantage function** is the difference between the expected future rewards and the baseline function. It tells the good or bad status and how good or bad that action was expected to be."
      ]
    },
    {
      "metadata": {
        "id": "x3rK1_lwrGQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#A3C for `Pong-v0` in OpenAI Gym"
      ]
    },
    {
      "metadata": {
        "id": "Y89til1GrO3g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import threading\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbbprjX4rkwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The input state image preprocessing function is required:"
      ]
    },
    {
      "metadata": {
        "id": "IlxRUPdZrfKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocessing_image(obs): #where I is the single frame of game as input\n",
        "  '''preprocessing 210x160x3 frame into 6400 (80x80) 1D float vector'''\n",
        "  #the values below have been precomputed through trial and error by OpenAI team\n",
        "  obs = obs[35:195] #crop image where it contains only the paddles\n",
        "  obs = obs[::2,::2,0] #downsample by factor of 2 and take only R channel\n",
        "  obs[obs==144] = 0 #erase background type 1\n",
        "  obs[obs==109] = 0 #erase background type 2\n",
        "  obs[obs!=0] = 1 #everything else set to 1\n",
        "  return obs.astype('float').ravel() #flatten to 1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uhw4kopwsmB5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set up the environment:"
      ]
    },
    {
      "metadata": {
        "id": "xpYuf1lWsn4Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "game_env = 'Pong-v0'\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "max_global_episodes = 100000\n",
        "global_network_scope = 'globalnet'\n",
        "global_iteration_update = 20\n",
        "gamma = 0.9\n",
        "beta = 0.0001\n",
        "lr_actor = 0.0001 # learning rate for actor\n",
        "lr_critic = 0.0001 # learning rate for critic\n",
        "global_running_rate = []\n",
        "global_episode = 0\n",
        "\n",
        "env = gym.make(game_env)\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZcY3NtOItJoi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the `actor-critic` class which contains the architecture of `actor` and `critic` network:"
      ]
    },
    {
      "metadata": {
        "id": "L7wzzxJmtRW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ActorCriticNetwork(object):\n",
        "  def __init__(self, scope, globalAC=None):\n",
        "    if scope == global_network_scope: # get global net\n",
        "      with tf.variable_scope(scope):\n",
        "        self.s = tf.placeholder(tf.float32, [None, 6400], 'state')\n",
        "        self.a_params, self.c_params = self._build_net(scope)[-2:]\n",
        "    else: # get local net, calculate losses\n",
        "      with tf.variable_scope(scope):\n",
        "        self.s = tf.placeholder(tf.float32, [None, 6400], 'state')\n",
        "        self.a_his = tf.placeholder(tf.int32, [None,], 'action')\n",
        "        self.v_target = tf.placeholder(tf.float32, [None, 1], 'target_vector')\n",
        "        \n",
        "        self.a_prob, self.v, self.a_params, self.c_params = self._build_net(scope)\n",
        "        \n",
        "        td = tf.subtract(self.v_target, self.v, name='temporal_difference_error')\n",
        "        with tf.name_scope('critic_loss'):\n",
        "          self.c_loss = tf.reduce_mean(tf.square(td))\n",
        "          \n",
        "        with tf.name_scope('actor_loss'):\n",
        "          log_prob = tf.reduce_sum(tf.log(self.a_prob) * tf.one_hot(self.a_his, num_actions, dtype=tf.float32), axis=1, keepdims=True),\n",
        "          exp_v = log_prob * td\n",
        "          entropy = -tf.reduce_sum(self.a_prob * tf.log(self.a_prob + 1e-5), axis=1, keepdims=True) #exploration\n",
        "          self.exp_v = beta * entropy + exp_v\n",
        "          self.a_loss = tf.reduce_mean(-self.exp_v)\n",
        "          \n",
        "        with tf.name_scope('local_grad'):\n",
        "          self.a_grads = tf.gradients(self.a_loss, self.a_params)\n",
        "          self.c_grads = tf.gradients(self.c_loss, self.c_params)\n",
        "          \n",
        "      with tf.name_scope('sync'):\n",
        "        with tf.name_scope('pull'):\n",
        "          self.pull_a_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.a_params, globalAC.a_params)]\n",
        "          self.pull_c_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.c_params, globalAC.c_params)]\n",
        "          \n",
        "        with tf.name_scope('push'):\n",
        "          self.update_a_op = actor_train.apply_gradients(zip(self.a_grads, globalAC.a_params))\n",
        "          self.update_c_op = critic_train.apply_gradients(zip(self.c_grads, globalAC.c_params))\n",
        "          \n",
        "          \n",
        "  def _build_net(self, scope):\n",
        "    w_init = tf.random_normal_initializer(0., .1)\n",
        "    \n",
        "    with tf.variable_scope('actor_network'):\n",
        "      l_a = tf.layers.dense(self.s, \n",
        "                            300, \n",
        "                            tf.nn.relu6, \n",
        "                            kernel_initializer=w_init, \n",
        "                            name='actor_layer')\n",
        "      a_prob = tf.layers.dense(l_a, \n",
        "                               num_actions, \n",
        "                               tf.nn.softmax,\n",
        "                               kernel_initializer=w_init,\n",
        "                               name='ap')\n",
        "      \n",
        "    with tf.variable_scope('critic_network'):\n",
        "      l_c = tf.layers.dense(self.s,\n",
        "                            100,\n",
        "                            tf.nn.relu6,\n",
        "                            kernel_initializer=w_init,\n",
        "                            name='critic_layer')\n",
        "      v = tf.layers.dense(l_c, \n",
        "                          1, \n",
        "                          kernel_initializer=w_init,\n",
        "                          name='v') # state value\n",
        "      \n",
        "      a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                   scope=scope + '/actor')\n",
        "      c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                   scope=scope + '/critic')\n",
        "      return a_prob, v, a_params, c_params\n",
        "    \n",
        "    \n",
        "  def update_global(self, feed_dict): # run local\n",
        "    session.run([self.update_a_op, self.update_c_op], feed_dict)\n",
        "    # local gradient applied to global net\n",
        "    \n",
        "    \n",
        "  def pull_global(self): # run local\n",
        "    session.run([self.pull_a_params_op, self.pull_c_params_op])\n",
        "    \n",
        "    \n",
        "  def choose_action(self, s): # run local\n",
        "    s = np.reshape(s, [-1])\n",
        "    prob_weights = session.run(self.a_prob,\n",
        "                               feed_dict={self.s: s[np.newaxis, :]})\n",
        "    action = np.random.choice(range(prob_weights.shape[1]),\n",
        "                              p=prob_weights.ravel()) # select action w.r.t the actions prob\n",
        "    return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ca2mIyBoyOcO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the `worker` class which represents the process in each thread:"
      ]
    },
    {
      "metadata": {
        "id": "vrnFm16ryTWc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Worker(object):\n",
        "  def __init__(self, name, globalAC):\n",
        "    self.env = gym.make(game_env).unwrapped\n",
        "    self.name = name\n",
        "    self.AC = ActorCriticNetwork(name, globalAC)\n",
        "    \n",
        "    \n",
        "  def work(self):\n",
        "    global global_running_rate, global_episode\n",
        "    total_step = 1\n",
        "    buffer_s, buffer_a, buffer_r = [], [], []\n",
        "    \n",
        "    while not coordinator.should_stop() and global_episode < max_global_episodes:\n",
        "      obs = self.env.reset()\n",
        "      s = preprocessing_image(obs)\n",
        "      ep_r = 0\n",
        "      \n",
        "      while True:\n",
        "        if self.name == 'W_0':\n",
        "          self.env.render(mode = 'rgb_array')\n",
        "        a = self.AC.choose_action(s)\n",
        "        \n",
        "        print(a.shape)\n",
        "        \n",
        "        obs_, r, done, info = self.env.step(a)\n",
        "        s_ = preprocessing_image(obs_)\n",
        "        if done and r<=0:\n",
        "          r = -20\n",
        "          \n",
        "        ep_r += r\n",
        "        buffer_s.append(np.reshape(s, [-1]))\n",
        "        buffer_a.append(a)\n",
        "        buffer_r.append(r)\n",
        "        \n",
        "        if total_step % global_iteration_update == 0 or done: \n",
        "          # update global and assign to local net\n",
        "          if done:\n",
        "            v_s = 0 # terminal\n",
        "          else:\n",
        "            s_ = np.reshape(s_, [-1])\n",
        "            v_s_ = session.run(self.AC.v, {self.AC.s: s_[np.newaxis, :]})[0, 0]\n",
        "            \n",
        "          buffer_v_target = []\n",
        "          for r in buffer_r[::-1]: #reverse buffer r\n",
        "            v_s_ = r + gamma * v_s_\n",
        "            buffer_v_target.append(v_s_)\n",
        "          buffer_v_target.reverse()\n",
        "          \n",
        "          buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.array(buffer_a), np.vstack(buffer_v_target)\n",
        "          feed_dict = {self.AC.s: buffer_s,\n",
        "                       self.AC.a_his: buffer_a,\n",
        "                       self.AC.v_target: buffer_v_target}\n",
        "          self.AC.update_global(feed_dict)\n",
        "          \n",
        "          buffer_s, buffer_a, buffer_r = [], [], []\n",
        "          self.AC.pull_global()\n",
        "          \n",
        "        s = s_\n",
        "        total_step += 1\n",
        "        \n",
        "        if done:\n",
        "          if len(global_running_rate) == 0: \n",
        "            # record running episode reward\n",
        "            global_running_rate.append(ep_r)\n",
        "          else:\n",
        "            global_running_rate.append(0.99 * global_running_rate[-1] + 0.01 * ep_r)\n",
        "          print(self.name,\n",
        "                'Ep:', global_episode,\n",
        "               '| Ep_r: %i' % global_running_rate[-1])\n",
        "          global_episode += 1\n",
        "          break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zNKUPSkR1ErS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can create the thread pool and assign workers to different threads:"
      ]
    },
    {
      "metadata": {
        "id": "SQOMAGuM1KS8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session = tf.Session()\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  actor_train = tf.train.RMSPropOptimizer(lr_actor, name='RMSPropOptimizerActor')\n",
        "  critic_train = tf.train.RMSPropOptimizer(lr_critic, name='RMSPropOptimizerCritic')\n",
        "  acn_global = ActorCriticNetwork(global_network_scope)\n",
        "  workers = []\n",
        "  \n",
        "  #create workers\n",
        "  for i in range(num_workers):\n",
        "    i_name = 'W_%i' % i # worker name\n",
        "    workers.append(Worker(i_name, acn_global))\n",
        "    \n",
        "coordinator = tf.train.Coordinator()\n",
        "session.run(tf.global_variables_initializer())\n",
        "\n",
        "worker_threads = []\n",
        "for worker in workers:\n",
        "  job = lambda: worker.work()\n",
        "  t = threading.Thread(target=job)\n",
        "  t.start()\n",
        "  worker_threads.append(t)\n",
        "coordinator.join(worker_threads)\n",
        "\n",
        "plt.plot(np.arange(len(global_running_rate)), global_running_rate)\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('Total moving reward')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}